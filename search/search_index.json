{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"api/const/","text":"LeastSquares Calculates coefficients of constraint via least squares using QR decomposition.","title":"Constraints"},{"location":"api/const/#deepymod_torch.model.constraint","text":"","title":"deepymod_torch.model.constraint"},{"location":"api/const/#deepymod_torch.model.constraint.LeastSquares","text":"Calculates coefficients of constraint via least squares using QR decomposition.","title":"LeastSquares"},{"location":"api/deepmod/","text":"This file contains the building blocks for the deepmod framework. These are all abstract classes and implement the flow logic, rather than the specifics. Constraint Constraint layer which applies sparsity mask and calculates the coefficients. forward ( self , input ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in deepymod_torch/model/deepmod.py 46 47 48 49 50 51 52 53 54 def forward ( self , input ): time_derivs , theta = input if self . sparsity_masks is None : self . sparsity_masks = [ torch . ones ( theta . shape [ 1 ], dtype = torch . bool ) for _ in torch . arange ( len ( time_derivs ))] sparse_thetas = self . apply_mask ( theta ) self . coeff_vectors = self . calculate_coeffs ( sparse_thetas , time_derivs ) return sparse_thetas , self . coeff_vectors DeepMoD Implements the deepmod model and connects all the separate elements. forward ( self , input ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in deepymod_torch/model/deepmod.py 18 19 20 21 22 def forward ( self , input ): prediction = self . func_approx ( input ) time_derivs , theta = self . library (( prediction , input )) sparse_thetas , constraint_coeffs = self . constraint (( time_derivs , theta )) return prediction , time_derivs , sparse_thetas , theta , constraint_coeffs Estimator Estimator layer which implements calculation of sparsity mask. forward ( self , theta , time_derivs ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in deepymod_torch/model/deepmod.py 68 69 70 71 72 73 74 def forward ( self , theta , time_derivs ): self . coeff_vectors = [ self . fit ( theta . detach () . cpu (), time_deriv . squeeze () . detach () . cpu ()) for time_deriv in time_derivs ] sparsity_masks = [ torch . tensor ( coeff_vector != 0.0 , dtype = torch . bool ) for coeff_vector in self . coeff_vectors ] return sparsity_masks Library Library layer which calculates and normalizes the library. forward ( self , input ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in deepymod_torch/model/deepmod.py 31 32 33 34 35 def forward ( self , input ): time_derivs , theta = self . library ( input ) self . norm = torch . norm ( theta , dim = 0 , keepdim = True ) normed_theta = theta / self . norm # we pass on the normed thetas return time_derivs , normed_theta","title":"DeepMoD"},{"location":"api/deepmod/#deepymod_torch.model.deepmod","text":"This file contains the building blocks for the deepmod framework. These are all abstract classes and implement the flow logic, rather than the specifics.","title":"deepymod_torch.model.deepmod"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.Constraint","text":"Constraint layer which applies sparsity mask and calculates the coefficients.","title":"Constraint"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.Constraint.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in deepymod_torch/model/deepmod.py 46 47 48 49 50 51 52 53 54 def forward ( self , input ): time_derivs , theta = input if self . sparsity_masks is None : self . sparsity_masks = [ torch . ones ( theta . shape [ 1 ], dtype = torch . bool ) for _ in torch . arange ( len ( time_derivs ))] sparse_thetas = self . apply_mask ( theta ) self . coeff_vectors = self . calculate_coeffs ( sparse_thetas , time_derivs ) return sparse_thetas , self . coeff_vectors","title":"forward()"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.DeepMoD","text":"Implements the deepmod model and connects all the separate elements.","title":"DeepMoD"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.DeepMoD.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in deepymod_torch/model/deepmod.py 18 19 20 21 22 def forward ( self , input ): prediction = self . func_approx ( input ) time_derivs , theta = self . library (( prediction , input )) sparse_thetas , constraint_coeffs = self . constraint (( time_derivs , theta )) return prediction , time_derivs , sparse_thetas , theta , constraint_coeffs","title":"forward()"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.Estimator","text":"Estimator layer which implements calculation of sparsity mask.","title":"Estimator"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.Estimator.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in deepymod_torch/model/deepmod.py 68 69 70 71 72 73 74 def forward ( self , theta , time_derivs ): self . coeff_vectors = [ self . fit ( theta . detach () . cpu (), time_deriv . squeeze () . detach () . cpu ()) for time_deriv in time_derivs ] sparsity_masks = [ torch . tensor ( coeff_vector != 0.0 , dtype = torch . bool ) for coeff_vector in self . coeff_vectors ] return sparsity_masks","title":"forward()"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.Library","text":"Library layer which calculates and normalizes the library.","title":"Library"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.Library.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in deepymod_torch/model/deepmod.py 31 32 33 34 35 def forward ( self , input ): time_derivs , theta = self . library ( input ) self . norm = torch . norm ( theta , dim = 0 , keepdim = True ) normed_theta = theta / self . norm # we pass on the normed thetas return time_derivs , normed_theta","title":"forward()"},{"location":"api/func_approx/","text":"NN Neural network function approximator. build_network ( self , n_in , n_hidden , n_out ) Constructs a feed-forward neural network. Parameters: Name Type Description Default n_in int Number of input features. required n_hidden list[int] Number of neurons in each layer. required n_out int Number of output features. required Returns: Type Description torch.Sequential Pytorch module Source code in deepymod_torch/model/func_approx.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def build_network ( self , n_in , n_hidden , n_out ): \"\"\" Constructs a feed-forward neural network. Args: n_in (int): Number of input features. n_hidden (list[int]): Number of neurons in each layer. n_out (int): Number of output features. Returns: torch.Sequential: Pytorch module \"\"\" network = [] architecture = [ n_in ] + n_hidden + [ n_out ] for layer_i , layer_j in zip ( architecture , architecture [ 1 :]): network . append ( nn . Linear ( layer_i , layer_j )) network . append ( nn . Tanh ()) network . pop () # get rid of last activation function return nn . Sequential ( * network ) forward ( self , input ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in deepymod_torch/model/func_approx.py 12 13 def forward ( self , input : torch . Tensor ) -> torch . Tensor : return self . network ( input )","title":"Function Approximators"},{"location":"api/func_approx/#deepymod_torch.model.func_approx","text":"","title":"deepymod_torch.model.func_approx"},{"location":"api/func_approx/#deepymod_torch.model.func_approx.NN","text":"Neural network function approximator.","title":"NN"},{"location":"api/func_approx/#deepymod_torch.model.func_approx.NN.build_network","text":"Constructs a feed-forward neural network. Parameters: Name Type Description Default n_in int Number of input features. required n_hidden list[int] Number of neurons in each layer. required n_out int Number of output features. required Returns: Type Description torch.Sequential Pytorch module Source code in deepymod_torch/model/func_approx.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def build_network ( self , n_in , n_hidden , n_out ): \"\"\" Constructs a feed-forward neural network. Args: n_in (int): Number of input features. n_hidden (list[int]): Number of neurons in each layer. n_out (int): Number of output features. Returns: torch.Sequential: Pytorch module \"\"\" network = [] architecture = [ n_in ] + n_hidden + [ n_out ] for layer_i , layer_j in zip ( architecture , architecture [ 1 :]): network . append ( nn . Linear ( layer_i , layer_j )) network . append ( nn . Tanh ()) network . pop () # get rid of last activation function return nn . Sequential ( * network )","title":"build_network()"},{"location":"api/func_approx/#deepymod_torch.model.func_approx.NN.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in deepymod_torch/model/func_approx.py 12 13 def forward ( self , input : torch . Tensor ) -> torch . Tensor : return self . network ( input )","title":"forward()"},{"location":"api/lib/","text":"Library1D Calculates library consisting of m-th order polynomials, n-th order derivatives and their cross terms. Library2D library ( self , input ) Constructs a library graph in 1D. Library config is dictionary with required terms. Source code in deepymod_torch/model/library.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def library ( self , input ): '''Constructs a library graph in 1D. Library config is dictionary with required terms. ''' prediction , data = input # Polynomial u = torch . ones_like ( prediction ) for order in np . arange ( 1 , self . poly_order + 1 ): u = torch . cat (( u , u [:, order - 1 : order ] * prediction ), dim = 1 ) # Gradients du = grad ( prediction , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] u_t = du [:, 0 : 1 ] u_x = du [:, 1 : 2 ] u_y = du [:, 2 : 3 ] du2 = grad ( u_x , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] u_xx = du2 [:, 1 : 2 ] u_xy = du2 [:, 2 : 3 ] u_yy = grad ( u_y , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ][:, 2 : 3 ] du = torch . cat (( torch . ones_like ( u_x ), u_x , u_y , u_xx , u_yy , u_xy ), dim = 1 ) samples = du . shape [ 0 ] # Bringing it together theta = torch . matmul ( u [:, :, None ], du [:, None , :]) . view ( samples , - 1 ) return [ u_t ], theta","title":"Libraries"},{"location":"api/lib/#deepymod_torch.model.library","text":"","title":"deepymod_torch.model.library"},{"location":"api/lib/#deepymod_torch.model.library.Library1D","text":"Calculates library consisting of m-th order polynomials, n-th order derivatives and their cross terms.","title":"Library1D"},{"location":"api/lib/#deepymod_torch.model.library.Library2D","text":"","title":"Library2D"},{"location":"api/lib/#deepymod_torch.model.library.Library2D.library","text":"Constructs a library graph in 1D. Library config is dictionary with required terms. Source code in deepymod_torch/model/library.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def library ( self , input ): '''Constructs a library graph in 1D. Library config is dictionary with required terms. ''' prediction , data = input # Polynomial u = torch . ones_like ( prediction ) for order in np . arange ( 1 , self . poly_order + 1 ): u = torch . cat (( u , u [:, order - 1 : order ] * prediction ), dim = 1 ) # Gradients du = grad ( prediction , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] u_t = du [:, 0 : 1 ] u_x = du [:, 1 : 2 ] u_y = du [:, 2 : 3 ] du2 = grad ( u_x , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] u_xx = du2 [:, 1 : 2 ] u_xy = du2 [:, 2 : 3 ] u_yy = grad ( u_y , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ][:, 2 : 3 ] du = torch . cat (( torch . ones_like ( u_x ), u_x , u_y , u_xx , u_yy , u_xy ), dim = 1 ) samples = du . shape [ 0 ] # Bringing it together theta = torch . matmul ( u [:, :, None ], du [:, None , :]) . view ( samples , - 1 ) return [ u_t ], theta","title":"library()"},{"location":"api/sparse/","text":"Sparsity estimators which can be plugged into deepmod. We keep the API in line with scikit learn (mostly), so scikit learn can also be plugged in. See scikitlearn.linear_models for applicable estimators. Base Simple wrapper class for scikit learn estimators for sparse estimator component. Clustering Performs additional thresholding by clustering on coefficient result from estimator. Basically a thin wrapper around the given estimator. Results are fitted to two groups: components to keep and components to throw. PDEFIND Implements PDEFIND as a sparse estimator. TrainSTLSQ ( X , y , alpha = 1e-05 , delta_threshold = 1.0 , max_iterations = 100 , test_size = 0.2 , random_state = 0 ) staticmethod Train STLSQ. Assumes data already normalized Source code in deepymod_torch/model/sparse_estimators.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 @staticmethod def TrainSTLSQ ( X , y , alpha = 1e-5 , delta_threshold = 1.0 , max_iterations = 100 , test_size = 0.2 , random_state = 0 ): '''Train STLSQ. Assumes data already normalized''' # Split data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = test_size , random_state = random_state ) # Set up the initial tolerance l0 penalty and estimates l0 = 1e-3 * np . linalg . cond ( X ) delta_t = delta_threshold # for interal use, can be updated # Initial estimate optimizer = STLSQ ( threshold = 0 , alpha = 0.0 , fit_intercept = False ) # Now similar to LSTSQ y_predict = optimizer . fit ( X_train , y_train ) . predict ( X_test ) min_loss = np . linalg . norm ( y_predict - y_test , 2 ) + l0 * np . count_nonzero ( optimizer . coef_ ) # Setting alpha and tolerance best_threshold = delta_t threshold = delta_t for iteration in np . arange ( max_iterations ): optimizer . set_params ( alpha = alpha , threshold = threshold ) y_predict = optimizer . fit ( X_train , y_train ) . predict ( X_test ) loss = np . linalg . norm ( y_predict - y_test , 2 ) + l0 * np . count_nonzero ( optimizer . coef_ ) if ( loss <= min_loss ) and not ( np . all ( optimizer . coef_ == 0 )): min_loss = loss best_threshold = threshold threshold += delta_threshold else : # if loss increases, we need to a) lower the current threshold and/or decrease step size new_lower_threshold = np . max ([ 0 , threshold - 2 * delta_t ]) delta_t = 2 * delta_t / ( max_iterations - iteration ) threshold = new_lower_threshold + delta_t optimizer . set_params ( alpha = alpha , threshold = best_threshold ) optimizer . fit ( X_train , y_train ) return optimizer . coef_ Threshold Performs additional thresholding on coefficient result from estimator. Basically a thin wrapper around the given estimator.","title":"Sparsity"},{"location":"api/sparse/#deepymod_torch.model.sparse_estimators","text":"Sparsity estimators which can be plugged into deepmod. We keep the API in line with scikit learn (mostly), so scikit learn can also be plugged in. See scikitlearn.linear_models for applicable estimators.","title":"deepymod_torch.model.sparse_estimators"},{"location":"api/sparse/#deepymod_torch.model.sparse_estimators.Base","text":"Simple wrapper class for scikit learn estimators for sparse estimator component.","title":"Base"},{"location":"api/sparse/#deepymod_torch.model.sparse_estimators.Clustering","text":"Performs additional thresholding by clustering on coefficient result from estimator. Basically a thin wrapper around the given estimator. Results are fitted to two groups: components to keep and components to throw.","title":"Clustering"},{"location":"api/sparse/#deepymod_torch.model.sparse_estimators.PDEFIND","text":"Implements PDEFIND as a sparse estimator.","title":"PDEFIND"},{"location":"api/sparse/#deepymod_torch.model.sparse_estimators.PDEFIND.TrainSTLSQ","text":"Train STLSQ. Assumes data already normalized Source code in deepymod_torch/model/sparse_estimators.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 @staticmethod def TrainSTLSQ ( X , y , alpha = 1e-5 , delta_threshold = 1.0 , max_iterations = 100 , test_size = 0.2 , random_state = 0 ): '''Train STLSQ. Assumes data already normalized''' # Split data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = test_size , random_state = random_state ) # Set up the initial tolerance l0 penalty and estimates l0 = 1e-3 * np . linalg . cond ( X ) delta_t = delta_threshold # for interal use, can be updated # Initial estimate optimizer = STLSQ ( threshold = 0 , alpha = 0.0 , fit_intercept = False ) # Now similar to LSTSQ y_predict = optimizer . fit ( X_train , y_train ) . predict ( X_test ) min_loss = np . linalg . norm ( y_predict - y_test , 2 ) + l0 * np . count_nonzero ( optimizer . coef_ ) # Setting alpha and tolerance best_threshold = delta_t threshold = delta_t for iteration in np . arange ( max_iterations ): optimizer . set_params ( alpha = alpha , threshold = threshold ) y_predict = optimizer . fit ( X_train , y_train ) . predict ( X_test ) loss = np . linalg . norm ( y_predict - y_test , 2 ) + l0 * np . count_nonzero ( optimizer . coef_ ) if ( loss <= min_loss ) and not ( np . all ( optimizer . coef_ == 0 )): min_loss = loss best_threshold = threshold threshold += delta_threshold else : # if loss increases, we need to a) lower the current threshold and/or decrease step size new_lower_threshold = np . max ([ 0 , threshold - 2 * delta_t ]) delta_t = 2 * delta_t / ( max_iterations - iteration ) threshold = new_lower_threshold + delta_t optimizer . set_params ( alpha = alpha , threshold = best_threshold ) optimizer . fit ( X_train , y_train ) return optimizer . coef_","title":"TrainSTLSQ()"},{"location":"api/sparse/#deepymod_torch.model.sparse_estimators.Threshold","text":"Performs additional thresholding on coefficient result from estimator. Basically a thin wrapper around the given estimator.","title":"Threshold"},{"location":"api/training/","text":"convergence Convergence Implements convergence criterium. Convergence is when change in patience epochs is smaller than delta. __call__ ( self , epoch , l1_norm ) special Calling Source code in deepymod_torch/training/convergence.py 18 19 20 21 22 23 24 25 26 27 28 29 30 def __call__ ( self , epoch , l1_norm ): ''' Calling ''' if self . start_l1 is None : self . start_l1 = l1_norm elif torch . abs ( self . start_l1 - l1_norm ) . item () < self . delta : self . counter += 1 if self . counter >= self . patience : self . converged = True else : self . start_l1 = l1_norm self . counter = 0 __init__ ( self , patience = 100 , delta = 0.05 ) special Initializes Source code in deepymod_torch/training/convergence.py 8 9 10 11 12 13 14 15 16 def __init__ ( self , patience = 100 , delta = 0.05 ): ''' Initializes ''' self . patience = patience self . delta = delta self . counter = 0 self . start_l1 = None self . converged = False sparsity_scheduler Periodic Controls when to apply sparsity. Initial_epoch is first time of appliance, then every periodicity epochs. training train ( model , data , target , optimizer , sparsity_scheduler , log_dir = None , max_iterations = 10000 , ** convergence_kwargs ) Function to train model. Source code in deepymod_torch/training/training.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def train ( model , data , target , optimizer , sparsity_scheduler , log_dir = None , max_iterations = 10000 , ** convergence_kwargs ): '''Function to train model.''' start_time = time . time () number_of_terms = [ coeff_vec . shape [ 0 ] for coeff_vec in model ( data )[ 3 ]] board = Tensorboard ( number_of_terms , log_dir ) # initializing custom tb board # Training convergence = Convergence ( ** convergence_kwargs ) print ( '| Iteration | Progress | Time remaining | Loss | MSE | Reg | L1 norm |' ) for iteration in torch . arange ( 0 , max_iterations + 1 ): # ================== Training Model ============================ prediction , time_derivs , sparse_thetas , thetas , constraint_coeffs = model ( data ) MSE = torch . mean (( prediction - target ) ** 2 , dim = 0 ) # loss per output Reg = torch . stack ([ torch . mean (( dt - theta @ coeff_vector ) ** 2 ) for dt , theta , coeff_vector in zip ( time_derivs , sparse_thetas , constraint_coeffs )]) loss = torch . sum ( 2 * torch . log ( 2 * pi * MSE ) + Reg / ( MSE + 1e-5 )) # 1e-5 for numerical stability # Optimizer step optimizer . zero_grad () loss . backward () optimizer . step () # ====================== Logging ======================= # Write progress to command line and tensorboard l1_norm = torch . stack ([ torch . sum ( torch . abs ( coeff_vector )) for coeff_vector in constraint_coeffs ]) if iteration % 25 == 0 : progress ( iteration , start_time , max_iterations , loss . item (), torch . sum ( MSE ) . item (), torch . sum ( Reg ) . item (), torch . sum ( l1_norm ) . item ()) # We pad the sparse vectors with zeros so they get written correctly) constraint_coeff_vectors = [ torch . zeros ( mask . size ()) . masked_scatter_ ( mask , coeff_vector . detach () . squeeze ()) for mask , coeff_vector in zip ( model . constraint . sparsity_masks , constraint_coeffs )] unscaled_constraint_coeff_vectors = [ torch . zeros ( mask . size ()) . masked_scatter_ ( mask , coeff_vector . detach () . squeeze ()) / model . library . norm . squeeze () for mask , coeff_vector in zip ( model . constraint . sparsity_masks , constraint_coeffs )] board . write ( iteration , loss , MSE , Reg , l1_norm , constraint_coeff_vectors , unscaled_constraint_coeff_vectors ) # ================== Validation and sparsity ============= # Updating sparsity and or convergence sparsity_scheduler ( iteration , torch . sum ( l1_norm )) if sparsity_scheduler . apply_sparsity is True : with torch . no_grad (): model . constraint . sparsity_masks = model . sparse_estimator ( thetas , time_derivs ) sparsity_scheduler . reset () print ( model . constraint . sparsity_masks ) # Checking convergence convergence ( iteration , torch . sum ( l1_norm )) if convergence . converged is True : print ( 'Algorithm converged. Stopping training.' ) break board . close ()","title":"Training"},{"location":"api/training/#deepymod_torch.training","text":"","title":"deepymod_torch.training"},{"location":"api/training/#deepymod_torch.training.convergence","text":"","title":"convergence"},{"location":"api/training/#deepymod_torch.training.convergence.Convergence","text":"Implements convergence criterium. Convergence is when change in patience epochs is smaller than delta.","title":"Convergence"},{"location":"api/training/#deepymod_torch.training.convergence.Convergence.__call__","text":"Calling Source code in deepymod_torch/training/convergence.py 18 19 20 21 22 23 24 25 26 27 28 29 30 def __call__ ( self , epoch , l1_norm ): ''' Calling ''' if self . start_l1 is None : self . start_l1 = l1_norm elif torch . abs ( self . start_l1 - l1_norm ) . item () < self . delta : self . counter += 1 if self . counter >= self . patience : self . converged = True else : self . start_l1 = l1_norm self . counter = 0","title":"__call__()"},{"location":"api/training/#deepymod_torch.training.convergence.Convergence.__init__","text":"Initializes Source code in deepymod_torch/training/convergence.py 8 9 10 11 12 13 14 15 16 def __init__ ( self , patience = 100 , delta = 0.05 ): ''' Initializes ''' self . patience = patience self . delta = delta self . counter = 0 self . start_l1 = None self . converged = False","title":"__init__()"},{"location":"api/training/#deepymod_torch.training.sparsity_scheduler","text":"","title":"sparsity_scheduler"},{"location":"api/training/#deepymod_torch.training.sparsity_scheduler.Periodic","text":"Controls when to apply sparsity. Initial_epoch is first time of appliance, then every periodicity epochs.","title":"Periodic"},{"location":"api/training/#deepymod_torch.training.training","text":"","title":"training"},{"location":"api/training/#deepymod_torch.training.training.train","text":"Function to train model. Source code in deepymod_torch/training/training.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def train ( model , data , target , optimizer , sparsity_scheduler , log_dir = None , max_iterations = 10000 , ** convergence_kwargs ): '''Function to train model.''' start_time = time . time () number_of_terms = [ coeff_vec . shape [ 0 ] for coeff_vec in model ( data )[ 3 ]] board = Tensorboard ( number_of_terms , log_dir ) # initializing custom tb board # Training convergence = Convergence ( ** convergence_kwargs ) print ( '| Iteration | Progress | Time remaining | Loss | MSE | Reg | L1 norm |' ) for iteration in torch . arange ( 0 , max_iterations + 1 ): # ================== Training Model ============================ prediction , time_derivs , sparse_thetas , thetas , constraint_coeffs = model ( data ) MSE = torch . mean (( prediction - target ) ** 2 , dim = 0 ) # loss per output Reg = torch . stack ([ torch . mean (( dt - theta @ coeff_vector ) ** 2 ) for dt , theta , coeff_vector in zip ( time_derivs , sparse_thetas , constraint_coeffs )]) loss = torch . sum ( 2 * torch . log ( 2 * pi * MSE ) + Reg / ( MSE + 1e-5 )) # 1e-5 for numerical stability # Optimizer step optimizer . zero_grad () loss . backward () optimizer . step () # ====================== Logging ======================= # Write progress to command line and tensorboard l1_norm = torch . stack ([ torch . sum ( torch . abs ( coeff_vector )) for coeff_vector in constraint_coeffs ]) if iteration % 25 == 0 : progress ( iteration , start_time , max_iterations , loss . item (), torch . sum ( MSE ) . item (), torch . sum ( Reg ) . item (), torch . sum ( l1_norm ) . item ()) # We pad the sparse vectors with zeros so they get written correctly) constraint_coeff_vectors = [ torch . zeros ( mask . size ()) . masked_scatter_ ( mask , coeff_vector . detach () . squeeze ()) for mask , coeff_vector in zip ( model . constraint . sparsity_masks , constraint_coeffs )] unscaled_constraint_coeff_vectors = [ torch . zeros ( mask . size ()) . masked_scatter_ ( mask , coeff_vector . detach () . squeeze ()) / model . library . norm . squeeze () for mask , coeff_vector in zip ( model . constraint . sparsity_masks , constraint_coeffs )] board . write ( iteration , loss , MSE , Reg , l1_norm , constraint_coeff_vectors , unscaled_constraint_coeff_vectors ) # ================== Validation and sparsity ============= # Updating sparsity and or convergence sparsity_scheduler ( iteration , torch . sum ( l1_norm )) if sparsity_scheduler . apply_sparsity is True : with torch . no_grad (): model . constraint . sparsity_masks = model . sparse_estimator ( thetas , time_derivs ) sparsity_scheduler . reset () print ( model . constraint . sparsity_masks ) # Checking convergence convergence ( iteration , torch . sum ( l1_norm )) if convergence . converged is True : print ( 'Algorithm converged. Stopping training.' ) break board . close ()","title":"train()"},{"location":"api/utils/","text":"We also make a package called phimal-utilities.","title":"Phimal-Utilities"},{"location":"background/deepmod/","text":"Deepmod Deepmod is really cool bla bla.","title":"DeepMoD"},{"location":"background/deepmod/#deepmod","text":"Deepmod is really cool bla bla.","title":"Deepmod"},{"location":"background/group_sparsity/","text":"Group sparsity","title":"Group sparsity"},{"location":"background/group_sparsity/#group-sparsity","text":"","title":"Group sparsity"},{"location":"examples/Burgers/PDE_Burgers/","text":"Example Burgers' equation In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the Burgers' equation. We start by importing the required libraries and setting the plotting style: # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD stuff from deepymod_torch.DeepMod import DeepMod from deepymod_torch.library_functions import library_1D_in from deepymod_torch.training import train_deepmod, train_mse # Settings for reproducibility np.random.seed(42) torch.manual_seed(0) %load_ext autoreload %autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Next, we prepare the dataset. data = np.load('data/burgers.npy', allow_pickle=True).item() print('Shape of grid:', data['x'].shape) Shape of grid: (256, 101) Let's plot it to get an idea of the data: fig, ax = plt.subplots() im = ax.contourf(data['x'], data['t'], np.real(data['u'])) ax.set_xlabel('x') ax.set_ylabel('t') fig.colorbar(mappable=im) plt.show() X = np.transpose((data['t'].flatten(), data['x'].flatten())) y = np.real(data['u']).reshape((data['u'].size, 1)) print(X.shape, y.shape) (25856, 2) (25856, 1) As we can see, $X$ has 2 dimensions, ${x, t}$, while $y$ has only one, ${u}$. Always explicity set the shape (i.e. $N\\times 1$, not $N$) or you'll get errors. This dataset is noiseless, so let's add $5\\%$ noise: noise_level = 0.05 y_noisy = y + noise_level * np.std(y) * np.random.randn(y[:,0].size, 1) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 1000 idx = np.random.permutation(y.shape[0]) X_train = torch.tensor(X[idx, :][:number_of_samples], dtype=torch.float32, requires_grad=True) y_train = torch.tensor(y_noisy[idx, :][:number_of_samples], dtype=torch.float32) print(X_train.shape, y_train.shape) torch.Size([1000, 2]) torch.Size([1000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig, axes = plt.subplots(ncols=3, figsize=(15, 4)) im0 = axes[0].contourf(data['x'], data['t'], np.real(data['u']), cmap='coolwarm') axes[0].set_xlabel('x') axes[0].set_ylabel('t') axes[0].set_title('Ground truth') im1 = axes[1].contourf(data['x'], data['t'], y_noisy.reshape(data['x'].shape), cmap='coolwarm') axes[1].set_xlabel('x') axes[1].set_title('Noisy') sampled = np.array([y_noisy[index, 0] if index in idx[:number_of_samples] else np.nan for index in np.arange(data['x'].size)]) sampled = np.rot90(sampled.reshape(data['x'].shape)) #array needs to be rotated because of imshow im2 = axes[2].imshow(sampled, aspect='auto', cmap='coolwarm') axes[2].set_xlabel('x') axes[2].set_title('Sampled') fig.colorbar(im1, ax=axes.ravel().tolist()) plt.show() Configuring DeepMoD We now setup the options for DeepMoD. The setup requires the dimensions of the neural network, a library function and some args for the library function: ## Running DeepMoD config = {'n_in': 2, 'hidden_dims': [20, 20, 20, 20, 20, 20], 'n_out': 1, 'library_function': library_1D_in, 'library_args':{'poly_order': 1, 'diff_order': 2}} Now we instantiate the model: model = DeepMod(**config) optimizer = torch.optim.Adam([{'params': model.network_parameters(), 'lr':0.001}, {'params': model.coeff_vector(), 'lr':0.005}]) Run DeepMoD We can now run DeepMoD using all the options we have set and the training data. We need to slightly preprocess the input data for the derivatives: train_deepmod(model, X_train, y_train, optimizer, 25000, {'l1': 1e-5}) | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 100 0.40% 371s 2.29e-02 1.64e-02 6.41e-03 1.25e-04 --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-21-790b1a88b6d5> in <module> ----> 1 train_deepmod(model, X_train, y_train, optimizer, 25000, {'l1': 1e-5}) ~/Documents/GitHub/New_DeepMod_Simple/DeePyMoD_torch/src/deepymod_torch/training.py in train_deepmod(model, data, target, optimizer, max_iterations, loss_func_args) 67 '''Performs full deepmod cycle: trains model, thresholds and trains again for unbiased estimate. Updates model in-place.''' 68 # Train first cycle and get prediction ---> 69 train(model, data, target, optimizer, max_iterations, loss_func_args) 70 prediction, time_deriv_list, sparse_theta_list, coeff_vector_list = model(data) 71 ~/Documents/GitHub/New_DeepMod_Simple/DeePyMoD_torch/src/deepymod_torch/training.py in train(model, data, target, optimizer, max_iterations, loss_func_args) 32 # Optimizer step 33 optimizer.zero_grad() ---> 34 loss.backward() 35 optimizer.step() 36 board.close() ~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph) 193 products. Defaults to ``False``. 194 \"\"\" --> 195 torch.autograd.backward(self, gradient, retain_graph, create_graph) 196 197 def register_hook(self, hook): ~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables) 97 Variable._execution_engine.run_backward( 98 tensors, grad_tensors, retain_graph, create_graph, ---> 99 allow_unreachable=True) # allow_unreachable flag 100 101 KeyboardInterrupt: Now that DeepMoD has converged, it has found the following numbers: print(model.fit.sparsity_mask) [tensor([2, 4])] print(model.fit.coeff_vector[0]) Parameter containing: tensor([[ 0.0974], [-0.9905]], requires_grad=True)","title":"Burgers"},{"location":"examples/Burgers/PDE_Burgers/#example-burgers-equation","text":"In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the Burgers' equation. We start by importing the required libraries and setting the plotting style: # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD stuff from deepymod_torch.DeepMod import DeepMod from deepymod_torch.library_functions import library_1D_in from deepymod_torch.training import train_deepmod, train_mse # Settings for reproducibility np.random.seed(42) torch.manual_seed(0) %load_ext autoreload %autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Next, we prepare the dataset. data = np.load('data/burgers.npy', allow_pickle=True).item() print('Shape of grid:', data['x'].shape) Shape of grid: (256, 101) Let's plot it to get an idea of the data: fig, ax = plt.subplots() im = ax.contourf(data['x'], data['t'], np.real(data['u'])) ax.set_xlabel('x') ax.set_ylabel('t') fig.colorbar(mappable=im) plt.show() X = np.transpose((data['t'].flatten(), data['x'].flatten())) y = np.real(data['u']).reshape((data['u'].size, 1)) print(X.shape, y.shape) (25856, 2) (25856, 1) As we can see, $X$ has 2 dimensions, ${x, t}$, while $y$ has only one, ${u}$. Always explicity set the shape (i.e. $N\\times 1$, not $N$) or you'll get errors. This dataset is noiseless, so let's add $5\\%$ noise: noise_level = 0.05 y_noisy = y + noise_level * np.std(y) * np.random.randn(y[:,0].size, 1) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 1000 idx = np.random.permutation(y.shape[0]) X_train = torch.tensor(X[idx, :][:number_of_samples], dtype=torch.float32, requires_grad=True) y_train = torch.tensor(y_noisy[idx, :][:number_of_samples], dtype=torch.float32) print(X_train.shape, y_train.shape) torch.Size([1000, 2]) torch.Size([1000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig, axes = plt.subplots(ncols=3, figsize=(15, 4)) im0 = axes[0].contourf(data['x'], data['t'], np.real(data['u']), cmap='coolwarm') axes[0].set_xlabel('x') axes[0].set_ylabel('t') axes[0].set_title('Ground truth') im1 = axes[1].contourf(data['x'], data['t'], y_noisy.reshape(data['x'].shape), cmap='coolwarm') axes[1].set_xlabel('x') axes[1].set_title('Noisy') sampled = np.array([y_noisy[index, 0] if index in idx[:number_of_samples] else np.nan for index in np.arange(data['x'].size)]) sampled = np.rot90(sampled.reshape(data['x'].shape)) #array needs to be rotated because of imshow im2 = axes[2].imshow(sampled, aspect='auto', cmap='coolwarm') axes[2].set_xlabel('x') axes[2].set_title('Sampled') fig.colorbar(im1, ax=axes.ravel().tolist()) plt.show()","title":"Example Burgers' equation"},{"location":"examples/Burgers/PDE_Burgers/#configuring-deepmod","text":"We now setup the options for DeepMoD. The setup requires the dimensions of the neural network, a library function and some args for the library function: ## Running DeepMoD config = {'n_in': 2, 'hidden_dims': [20, 20, 20, 20, 20, 20], 'n_out': 1, 'library_function': library_1D_in, 'library_args':{'poly_order': 1, 'diff_order': 2}} Now we instantiate the model: model = DeepMod(**config) optimizer = torch.optim.Adam([{'params': model.network_parameters(), 'lr':0.001}, {'params': model.coeff_vector(), 'lr':0.005}])","title":"Configuring DeepMoD"},{"location":"examples/Burgers/PDE_Burgers/#run-deepmod","text":"We can now run DeepMoD using all the options we have set and the training data. We need to slightly preprocess the input data for the derivatives: train_deepmod(model, X_train, y_train, optimizer, 25000, {'l1': 1e-5}) | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 100 0.40% 371s 2.29e-02 1.64e-02 6.41e-03 1.25e-04 --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-21-790b1a88b6d5> in <module> ----> 1 train_deepmod(model, X_train, y_train, optimizer, 25000, {'l1': 1e-5}) ~/Documents/GitHub/New_DeepMod_Simple/DeePyMoD_torch/src/deepymod_torch/training.py in train_deepmod(model, data, target, optimizer, max_iterations, loss_func_args) 67 '''Performs full deepmod cycle: trains model, thresholds and trains again for unbiased estimate. Updates model in-place.''' 68 # Train first cycle and get prediction ---> 69 train(model, data, target, optimizer, max_iterations, loss_func_args) 70 prediction, time_deriv_list, sparse_theta_list, coeff_vector_list = model(data) 71 ~/Documents/GitHub/New_DeepMod_Simple/DeePyMoD_torch/src/deepymod_torch/training.py in train(model, data, target, optimizer, max_iterations, loss_func_args) 32 # Optimizer step 33 optimizer.zero_grad() ---> 34 loss.backward() 35 optimizer.step() 36 board.close() ~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph) 193 products. Defaults to ``False``. 194 \"\"\" --> 195 torch.autograd.backward(self, gradient, retain_graph, create_graph) 196 197 def register_hook(self, hook): ~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables) 97 Variable._execution_engine.run_backward( 98 tensors, grad_tensors, retain_graph, create_graph, ---> 99 allow_unreachable=True) # allow_unreachable flag 100 101 KeyboardInterrupt: Now that DeepMoD has converged, it has found the following numbers: print(model.fit.sparsity_mask) [tensor([2, 4])] print(model.fit.coeff_vector[0]) Parameter containing: tensor([[ 0.0974], [-0.9905]], requires_grad=True)","title":"Run DeepMoD"},{"location":"examples/kdv/PDE_KdV/","text":"Example Korteweg de Vries equation In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the Burgers' equation. We start by importing the required libraries and setting the plotting style: # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD stuff from deepymod_torch.DeepMod import DeepMod from deepymod_torch.library_functions import library_1D_in from deepymod_torch.training import train_deepmod, train_mse # Settings for reproducibility np.random.seed(42) torch.manual_seed(0) %load_ext autoreload %autoreload 2 Next, we prepare the dataset. data = np.load('data/kdv.npy', allow_pickle=True).item() print('Shape of grid:', data['x'].shape) Shape of grid: (512, 201) Let's plot it to get an idea of the data: fig, ax = plt.subplots() im = ax.contourf(data['x'], data['t'], np.real(data['u'])) ax.set_xlabel('x') ax.set_ylabel('t') fig.colorbar(mappable=im) plt.show() X = np.transpose((data['t'].flatten(), data['x'].flatten())) y = np.real(data['u']).reshape((data['u'].size, 1)) print(X.shape, y.shape) (102912, 2) (102912, 1) As we can see, $X$ has 2 dimensions, ${x, t}$, while $y$ has only one, ${u}$. Always explicity set the shape (i.e. $N\\times 1$, not $N$) or you'll get errors. This dataset is noiseless, so let's add $5\\%$ noise: noise_level = 0.05 y_noisy = y + noise_level * np.std(y) * np.random.randn(y[:,0].size, 1) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 1000 idx = np.random.permutation(y.shape[0]) X_train = torch.tensor(X[idx, :][:number_of_samples], dtype=torch.float32, requires_grad=True) y_train = torch.tensor(y_noisy[idx, :][:number_of_samples], dtype=torch.float32) print(X_train.shape, y_train.shape) torch.Size([1000, 2]) torch.Size([1000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig, axes = plt.subplots(ncols=3, figsize=(15, 4)) im0 = axes[0].contourf(data['x'], data['t'], np.real(data['u']), cmap='coolwarm') axes[0].set_xlabel('x') axes[0].set_ylabel('t') axes[0].set_title('Ground truth') im1 = axes[1].contourf(data['x'], data['t'], y_noisy.reshape(data['x'].shape), cmap='coolwarm') axes[1].set_xlabel('x') axes[1].set_title('Noisy') sampled = np.array([y_noisy[index, 0] if index in idx[:number_of_samples] else np.nan for index in np.arange(data['x'].size)]) sampled = np.rot90(sampled.reshape(data['x'].shape)) #array needs to be rotated because of imshow im2 = axes[2].imshow(sampled, aspect='auto', cmap='coolwarm') axes[2].set_xlabel('x') axes[2].set_title('Sampled') fig.colorbar(im1, ax=axes.ravel().tolist()) plt.show() Configuring DeepMoD We now setup the options for DeepMoD. The setup requires the dimensions of the neural network, a library function and some args for the library function: ## Running DeepMoD config = {'n_in': 2, 'hidden_dims': [20, 20, 20, 20, 20, 20], 'n_out': 1, 'library_function': library_1D_in, 'library_args':{'poly_order': 1, 'diff_order': 3}} Now we instantiate the model: model = DeepMod(**config) optimizer = torch.optim.Adam([{'params': model.network_parameters(), 'lr':0.001}, {'params': model.coeff_vector(), 'lr':0.0025}],betas=(0.99, 0.99)) Run DeepMoD We can now run DeepMoD using all the options we have set and the training data. We need to slightly preprocess the input data for the derivatives: train_deepmod(model, X_train, y_train, optimizer, 25000, {'l1': 1e-5}) | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 25000 100.00% 0s 5.83e-05 3.13e-05 4.58e-06 2.24e-05 [Parameter containing: tensor([[-0.7742], [-4.7608]], requires_grad=True)] [tensor([3, 5])] | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 25000 100.00% 0s 9.18e-05 2.87e-05 6.32e-05 0.00e+00 Now that DeepMoD has converged, it has found the following numbers: print(model.fit.sparsity_mask) [tensor([3, 5])] print(model.fit.coeff_vector[0]) Parameter containing: tensor([[-0.7742], [-4.7608]], requires_grad=True)","title":"Korteweg-de Vries"},{"location":"examples/kdv/PDE_KdV/#example-korteweg-de-vries-equation","text":"In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the Burgers' equation. We start by importing the required libraries and setting the plotting style: # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD stuff from deepymod_torch.DeepMod import DeepMod from deepymod_torch.library_functions import library_1D_in from deepymod_torch.training import train_deepmod, train_mse # Settings for reproducibility np.random.seed(42) torch.manual_seed(0) %load_ext autoreload %autoreload 2 Next, we prepare the dataset. data = np.load('data/kdv.npy', allow_pickle=True).item() print('Shape of grid:', data['x'].shape) Shape of grid: (512, 201) Let's plot it to get an idea of the data: fig, ax = plt.subplots() im = ax.contourf(data['x'], data['t'], np.real(data['u'])) ax.set_xlabel('x') ax.set_ylabel('t') fig.colorbar(mappable=im) plt.show() X = np.transpose((data['t'].flatten(), data['x'].flatten())) y = np.real(data['u']).reshape((data['u'].size, 1)) print(X.shape, y.shape) (102912, 2) (102912, 1) As we can see, $X$ has 2 dimensions, ${x, t}$, while $y$ has only one, ${u}$. Always explicity set the shape (i.e. $N\\times 1$, not $N$) or you'll get errors. This dataset is noiseless, so let's add $5\\%$ noise: noise_level = 0.05 y_noisy = y + noise_level * np.std(y) * np.random.randn(y[:,0].size, 1) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 1000 idx = np.random.permutation(y.shape[0]) X_train = torch.tensor(X[idx, :][:number_of_samples], dtype=torch.float32, requires_grad=True) y_train = torch.tensor(y_noisy[idx, :][:number_of_samples], dtype=torch.float32) print(X_train.shape, y_train.shape) torch.Size([1000, 2]) torch.Size([1000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig, axes = plt.subplots(ncols=3, figsize=(15, 4)) im0 = axes[0].contourf(data['x'], data['t'], np.real(data['u']), cmap='coolwarm') axes[0].set_xlabel('x') axes[0].set_ylabel('t') axes[0].set_title('Ground truth') im1 = axes[1].contourf(data['x'], data['t'], y_noisy.reshape(data['x'].shape), cmap='coolwarm') axes[1].set_xlabel('x') axes[1].set_title('Noisy') sampled = np.array([y_noisy[index, 0] if index in idx[:number_of_samples] else np.nan for index in np.arange(data['x'].size)]) sampled = np.rot90(sampled.reshape(data['x'].shape)) #array needs to be rotated because of imshow im2 = axes[2].imshow(sampled, aspect='auto', cmap='coolwarm') axes[2].set_xlabel('x') axes[2].set_title('Sampled') fig.colorbar(im1, ax=axes.ravel().tolist()) plt.show()","title":"Example Korteweg de Vries equation"},{"location":"examples/kdv/PDE_KdV/#configuring-deepmod","text":"We now setup the options for DeepMoD. The setup requires the dimensions of the neural network, a library function and some args for the library function: ## Running DeepMoD config = {'n_in': 2, 'hidden_dims': [20, 20, 20, 20, 20, 20], 'n_out': 1, 'library_function': library_1D_in, 'library_args':{'poly_order': 1, 'diff_order': 3}} Now we instantiate the model: model = DeepMod(**config) optimizer = torch.optim.Adam([{'params': model.network_parameters(), 'lr':0.001}, {'params': model.coeff_vector(), 'lr':0.0025}],betas=(0.99, 0.99))","title":"Configuring DeepMoD"},{"location":"examples/kdv/PDE_KdV/#run-deepmod","text":"We can now run DeepMoD using all the options we have set and the training data. We need to slightly preprocess the input data for the derivatives: train_deepmod(model, X_train, y_train, optimizer, 25000, {'l1': 1e-5}) | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 25000 100.00% 0s 5.83e-05 3.13e-05 4.58e-06 2.24e-05 [Parameter containing: tensor([[-0.7742], [-4.7608]], requires_grad=True)] [tensor([3, 5])] | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 25000 100.00% 0s 9.18e-05 2.87e-05 6.32e-05 0.00e+00 Now that DeepMoD has converged, it has found the following numbers: print(model.fit.sparsity_mask) [tensor([3, 5])] print(model.fit.coeff_vector[0]) Parameter containing: tensor([[-0.7742], [-4.7608]], requires_grad=True)","title":"Run DeepMoD"}]}