{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"api/const/","text":"LeastSquares [summary] !!! args Constraint ([type]): [description] calculate_coeffs ( self , sparse_thetas , time_derivs ) [summary] Parameters: Name Type Description Default sparse_thetas <function NewType.<locals>.new_type at 0x10d9c3c20> [description] required time_derivs <function NewType.<locals>.new_type at 0x10d9c3c20> [description] required Returns: Type Description <function NewType.<locals>.new_type at 0x10d9c3c20> Source code in deepymod_torch/model/constraint.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def calculate_coeffs ( self , sparse_thetas : TensorList , time_derivs : TensorList ) -> TensorList : \"\"\"[summary] Args: sparse_thetas (TensorList): [description] time_derivs (TensorList): [description] Returns: [type]: [description] \"\"\" opt_coeff = [] for theta , dt in zip ( sparse_thetas , time_derivs ): Q , R = torch . qr ( theta ) # solution of lst. sq. by QR decomp. opt_coeff . append ( torch . inverse ( R ) @ Q . T @ dt ) return opt_coeff","title":"Constraints"},{"location":"api/const/#deepymod_torch.model.constraint","text":"","title":"deepymod_torch.model.constraint"},{"location":"api/const/#deepymod_torch.model.constraint.LeastSquares","text":"[summary] !!! args Constraint ([type]): [description]","title":"LeastSquares"},{"location":"api/const/#deepymod_torch.model.constraint.LeastSquares.calculate_coeffs","text":"[summary] Parameters: Name Type Description Default sparse_thetas <function NewType.<locals>.new_type at 0x10d9c3c20> [description] required time_derivs <function NewType.<locals>.new_type at 0x10d9c3c20> [description] required Returns: Type Description <function NewType.<locals>.new_type at 0x10d9c3c20> Source code in deepymod_torch/model/constraint.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def calculate_coeffs ( self , sparse_thetas : TensorList , time_derivs : TensorList ) -> TensorList : \"\"\"[summary] Args: sparse_thetas (TensorList): [description] time_derivs (TensorList): [description] Returns: [type]: [description] \"\"\" opt_coeff = [] for theta , dt in zip ( sparse_thetas , time_derivs ): Q , R = torch . qr ( theta ) # solution of lst. sq. by QR decomp. opt_coeff . append ( torch . inverse ( R ) @ Q . T @ dt ) return opt_coeff","title":"calculate_coeffs()"},{"location":"api/deepmod/","text":"This file contains the building blocks for the deepmod framework. These are all abstract classes and implement the flow logic, rather than the specifics. Constraint [summary] !!! args nn ([type]): [description] apply_mask ( self , thetas ) [summary] Parameters: Name Type Description Default thetas <function NewType.<locals>.new_type at 0x10d9c3c20> [description] required Returns: Type Description <function NewType.<locals>.new_type at 0x10d9c3c20> TensorList: [description] Source code in deepymod_torch/model/deepmod.py 41 42 43 44 45 46 47 48 49 50 51 def apply_mask ( self , thetas : TensorList ) -> TensorList : \"\"\"[summary] Args: thetas (TensorList): [description] Returns: TensorList: [description] \"\"\" sparse_thetas = [ theta [:, sparsity_mask ] for theta , sparsity_mask in zip ( thetas , self . sparsity_masks )] return sparse_thetas forward ( self , input ) [summary] Parameters: Name Type Description Default input Tuple[TensorList, TensorList] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod_torch/model/deepmod.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def forward ( self , input : Tuple [ TensorList , TensorList ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (Tuple[TensorList, TensorList]): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" time_derivs , thetas = input if self . sparsity_masks is None : self . sparsity_masks = [ torch . ones ( theta . shape [ 1 ], dtype = torch . bool ) for theta in thetas ] sparse_thetas = self . apply_mask ( thetas ) self . coeff_vectors = self . calculate_coeffs ( sparse_thetas , time_derivs ) return sparse_thetas , self . coeff_vectors DeepMoD [summary] !!! args nn ([type]): [description] forward ( self , input ) [summary] Parameters: Name Type Description Default input Tensor [description] required Returns: Type Description Tuple[TensorList, TensorList, TensorList, TensorList, TensorList] Tuple[TensorList, TensorList, TensorList, TensorList, TensorList]: [description] Source code in deepymod_torch/model/deepmod.py 137 138 139 140 141 142 143 144 145 146 147 148 149 def forward ( self , input : torch . Tensor ) -> Tuple [ TensorList , TensorList , TensorList , TensorList , TensorList ]: \"\"\"[summary] Args: input (torch.Tensor): [description] Returns: Tuple[TensorList, TensorList, TensorList, TensorList, TensorList]: [description] \"\"\" prediction = self . func_approx ( input ) time_derivs , thetas = self . library (( prediction , input )) sparse_thetas , constraint_coeffs = self . constraint (( time_derivs , thetas )) return prediction , time_derivs , sparse_thetas , thetas , constraint_coeffs Estimator [summary] !!! args nn ([type]): [description] forward ( self , thetas , time_derivs ) [summary] Parameters: Name Type Description Default thetas <function NewType.<locals>.new_type at 0x10d9c3c20> [description] required time_derivs <function NewType.<locals>.new_type at 0x10d9c3c20> [description] required Returns: Type Description <function NewType.<locals>.new_type at 0x10d9c3c20> TensorList: [description] Source code in deepymod_torch/model/deepmod.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def forward ( self , thetas : TensorList , time_derivs : TensorList ) -> TensorList : \"\"\"[summary] Args: thetas (TensorList): [description] time_derivs (TensorList): [description] Returns: TensorList: [description] \"\"\" self . coeff_vectors = [ self . fit ( theta . detach () . cpu (), time_deriv . squeeze () . detach () . cpu ()) for theta , time_deriv in zip ( thetas , time_derivs )] sparsity_masks = [ torch . tensor ( coeff_vector != 0.0 , dtype = torch . bool ) for coeff_vector in self . coeff_vectors ] return sparsity_masks Library [summary] !!! args nn ([type]): [description] forward ( self , input ) [summary] Parameters: Name Type Description Default input Tuple[TensorList, TensorList] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod_torch/model/deepmod.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def forward ( self , input : Tuple [ TensorList , TensorList ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (torch.Tensor): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" time_derivs , thetas = self . library ( input ) theta_norms = [ torch . norm ( theta , dim = 0 , keepdim = True ) for theta in thetas ] time_deriv_norms = [ torch . norm ( dt ) for dt in time_derivs ] normed_thetas = [ theta / norm for theta , norm in zip ( thetas , theta_norms )] normed_time_derivs = [ dt / norm for dt , norm in zip ( time_derivs , time_deriv_norms )] self . norms = [ theta_norm / dt_norm for theta_norm , dt_norm in zip ( theta_norms , time_deriv_norms )] return normed_time_derivs , normed_thetas","title":"DeepMoD"},{"location":"api/deepmod/#deepymod_torch.model.deepmod","text":"This file contains the building blocks for the deepmod framework. These are all abstract classes and implement the flow logic, rather than the specifics.","title":"deepymod_torch.model.deepmod"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.Constraint","text":"[summary] !!! args nn ([type]): [description]","title":"Constraint"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.Constraint.apply_mask","text":"[summary] Parameters: Name Type Description Default thetas <function NewType.<locals>.new_type at 0x10d9c3c20> [description] required Returns: Type Description <function NewType.<locals>.new_type at 0x10d9c3c20> TensorList: [description] Source code in deepymod_torch/model/deepmod.py 41 42 43 44 45 46 47 48 49 50 51 def apply_mask ( self , thetas : TensorList ) -> TensorList : \"\"\"[summary] Args: thetas (TensorList): [description] Returns: TensorList: [description] \"\"\" sparse_thetas = [ theta [:, sparsity_mask ] for theta , sparsity_mask in zip ( thetas , self . sparsity_masks )] return sparse_thetas","title":"apply_mask()"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.Constraint.forward","text":"[summary] Parameters: Name Type Description Default input Tuple[TensorList, TensorList] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod_torch/model/deepmod.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def forward ( self , input : Tuple [ TensorList , TensorList ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (Tuple[TensorList, TensorList]): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" time_derivs , thetas = input if self . sparsity_masks is None : self . sparsity_masks = [ torch . ones ( theta . shape [ 1 ], dtype = torch . bool ) for theta in thetas ] sparse_thetas = self . apply_mask ( thetas ) self . coeff_vectors = self . calculate_coeffs ( sparse_thetas , time_derivs ) return sparse_thetas , self . coeff_vectors","title":"forward()"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.DeepMoD","text":"[summary] !!! args nn ([type]): [description]","title":"DeepMoD"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.DeepMoD.forward","text":"[summary] Parameters: Name Type Description Default input Tensor [description] required Returns: Type Description Tuple[TensorList, TensorList, TensorList, TensorList, TensorList] Tuple[TensorList, TensorList, TensorList, TensorList, TensorList]: [description] Source code in deepymod_torch/model/deepmod.py 137 138 139 140 141 142 143 144 145 146 147 148 149 def forward ( self , input : torch . Tensor ) -> Tuple [ TensorList , TensorList , TensorList , TensorList , TensorList ]: \"\"\"[summary] Args: input (torch.Tensor): [description] Returns: Tuple[TensorList, TensorList, TensorList, TensorList, TensorList]: [description] \"\"\" prediction = self . func_approx ( input ) time_derivs , thetas = self . library (( prediction , input )) sparse_thetas , constraint_coeffs = self . constraint (( time_derivs , thetas )) return prediction , time_derivs , sparse_thetas , thetas , constraint_coeffs","title":"forward()"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.Estimator","text":"[summary] !!! args nn ([type]): [description]","title":"Estimator"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.Estimator.forward","text":"[summary] Parameters: Name Type Description Default thetas <function NewType.<locals>.new_type at 0x10d9c3c20> [description] required time_derivs <function NewType.<locals>.new_type at 0x10d9c3c20> [description] required Returns: Type Description <function NewType.<locals>.new_type at 0x10d9c3c20> TensorList: [description] Source code in deepymod_torch/model/deepmod.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def forward ( self , thetas : TensorList , time_derivs : TensorList ) -> TensorList : \"\"\"[summary] Args: thetas (TensorList): [description] time_derivs (TensorList): [description] Returns: TensorList: [description] \"\"\" self . coeff_vectors = [ self . fit ( theta . detach () . cpu (), time_deriv . squeeze () . detach () . cpu ()) for theta , time_deriv in zip ( thetas , time_derivs )] sparsity_masks = [ torch . tensor ( coeff_vector != 0.0 , dtype = torch . bool ) for coeff_vector in self . coeff_vectors ] return sparsity_masks","title":"forward()"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.Library","text":"[summary] !!! args nn ([type]): [description]","title":"Library"},{"location":"api/deepmod/#deepymod_torch.model.deepmod.Library.forward","text":"[summary] Parameters: Name Type Description Default input Tuple[TensorList, TensorList] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod_torch/model/deepmod.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def forward ( self , input : Tuple [ TensorList , TensorList ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (torch.Tensor): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" time_derivs , thetas = self . library ( input ) theta_norms = [ torch . norm ( theta , dim = 0 , keepdim = True ) for theta in thetas ] time_deriv_norms = [ torch . norm ( dt ) for dt in time_derivs ] normed_thetas = [ theta / norm for theta , norm in zip ( thetas , theta_norms )] normed_time_derivs = [ dt / norm for dt , norm in zip ( time_derivs , time_deriv_norms )] self . norms = [ theta_norm / dt_norm for theta_norm , dt_norm in zip ( theta_norms , time_deriv_norms )] return normed_time_derivs , normed_thetas","title":"forward()"},{"location":"api/func_approx/","text":"NN [summary] !!! args nn ([type]): [description] build_network ( self , n_in , n_hidden , n_out ) Constructs a feed-forward neural network. Parameters: Name Type Description Default n_in int Number of input features. required n_hidden List[int] Number of neurons in each layer. required n_out int Number of output features. required Returns: Type Description Sequential torch.Sequential: Pytorch module Source code in deepymod_torch/model/func_approx.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def build_network ( self , n_in : int , n_hidden : List [ int ], n_out : int ) -> torch . nn . Sequential : \"\"\" Constructs a feed-forward neural network. Args: n_in (int): Number of input features. n_hidden (list[int]): Number of neurons in each layer. n_out (int): Number of output features. Returns: torch.Sequential: Pytorch module \"\"\" network = [] architecture = [ n_in ] + n_hidden + [ n_out ] for layer_i , layer_j in zip ( architecture , architecture [ 1 :]): network . append ( nn . Linear ( layer_i , layer_j )) network . append ( nn . Tanh ()) network . pop () # get rid of last activation function return nn . Sequential ( * network ) forward ( self , input ) [summary] Parameters: Name Type Description Default input Tensor [description] required Returns: Type Description Tensor torch.Tensor: [description] Source code in deepymod_torch/model/func_approx.py 16 17 18 19 20 21 22 23 24 25 def forward ( self , input : torch . Tensor ) -> torch . Tensor : \"\"\"[summary] Args: input (torch.Tensor): [description] Returns: torch.Tensor: [description] \"\"\" return self . network ( input )","title":"Function Approximators"},{"location":"api/func_approx/#deepymod_torch.model.func_approx","text":"","title":"deepymod_torch.model.func_approx"},{"location":"api/func_approx/#deepymod_torch.model.func_approx.NN","text":"[summary] !!! args nn ([type]): [description]","title":"NN"},{"location":"api/func_approx/#deepymod_torch.model.func_approx.NN.build_network","text":"Constructs a feed-forward neural network. Parameters: Name Type Description Default n_in int Number of input features. required n_hidden List[int] Number of neurons in each layer. required n_out int Number of output features. required Returns: Type Description Sequential torch.Sequential: Pytorch module Source code in deepymod_torch/model/func_approx.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def build_network ( self , n_in : int , n_hidden : List [ int ], n_out : int ) -> torch . nn . Sequential : \"\"\" Constructs a feed-forward neural network. Args: n_in (int): Number of input features. n_hidden (list[int]): Number of neurons in each layer. n_out (int): Number of output features. Returns: torch.Sequential: Pytorch module \"\"\" network = [] architecture = [ n_in ] + n_hidden + [ n_out ] for layer_i , layer_j in zip ( architecture , architecture [ 1 :]): network . append ( nn . Linear ( layer_i , layer_j )) network . append ( nn . Tanh ()) network . pop () # get rid of last activation function return nn . Sequential ( * network )","title":"build_network()"},{"location":"api/func_approx/#deepymod_torch.model.func_approx.NN.forward","text":"[summary] Parameters: Name Type Description Default input Tensor [description] required Returns: Type Description Tensor torch.Tensor: [description] Source code in deepymod_torch/model/func_approx.py 16 17 18 19 20 21 22 23 24 25 def forward ( self , input : torch . Tensor ) -> torch . Tensor : \"\"\"[summary] Args: input (torch.Tensor): [description] Returns: torch.Tensor: [description] \"\"\" return self . network ( input )","title":"forward()"},{"location":"api/lib/","text":"Library1D [summary] !!! args Library ([type]): [description] library ( self , input ) [summary] Parameters: Name Type Description Default input Tuple[torch.Tensor, torch.Tensor] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod_torch/model/library.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def library ( self , input : Tuple [ torch . Tensor , torch . Tensor ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (Tuple[torch.Tensor, torch.Tensor]): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" prediction , data = input poly_list = [] deriv_list = [] time_deriv_list = [] # Creating lists for all outputs for output in np . arange ( prediction . shape [ 1 ]): time_deriv , du = library_deriv ( data , prediction [:, output : output + 1 ], self . diff_order ) u = library_poly ( prediction [:, output : output + 1 ], self . poly_order ) poly_list . append ( u ) deriv_list . append ( du ) time_deriv_list . append ( time_deriv ) samples = time_deriv_list [ 0 ] . shape [ 0 ] total_terms = poly_list [ 0 ] . shape [ 1 ] * deriv_list [ 0 ] . shape [ 1 ] # Calculating theta if len ( poly_list ) == 1 : # If we have a single output, we simply calculate and flatten matrix product # between polynomials and derivatives to get library theta = torch . matmul ( poly_list [ 0 ][:, :, None ], deriv_list [ 0 ][:, None , :]) . view ( samples , total_terms ) else : theta_uv = reduce (( lambda x , y : ( x [:, :, None ] @ y [:, None , :]) . view ( samples , - 1 )), poly_list ) # calculate all unique combinations of derivatives theta_dudv = torch . cat ([ torch . matmul ( du [:, :, None ], dv [:, None , :]) . view ( samples , - 1 )[:, 1 :] for du , dv in combinations ( deriv_list , 2 )], 1 ) theta = torch . cat ([ theta_uv , theta_dudv ], dim = 1 ) return time_deriv_list , [ theta ] Library2D [summary] !!! args Library ([type]): [description] library ( self , input ) [summary] Parameters: Name Type Description Default input Tuple[torch.Tensor, torch.Tensor] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod_torch/model/library.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def library ( self , input : Tuple [ torch . Tensor , torch . Tensor ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (Tuple[torch.Tensor, torch.Tensor]): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" prediction , data = input # Polynomial u = torch . ones_like ( prediction ) for order in np . arange ( 1 , self . poly_order + 1 ): u = torch . cat (( u , u [:, order - 1 : order ] * prediction ), dim = 1 ) # Gradients du = grad ( prediction , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] u_t = du [:, 0 : 1 ] u_x = du [:, 1 : 2 ] u_y = du [:, 2 : 3 ] du2 = grad ( u_x , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] u_xx = du2 [:, 1 : 2 ] u_xy = du2 [:, 2 : 3 ] u_yy = grad ( u_y , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ][:, 2 : 3 ] du = torch . cat (( torch . ones_like ( u_x ), u_x , u_y , u_xx , u_yy , u_xy ), dim = 1 ) samples = du . shape [ 0 ] # Bringing it together theta = torch . matmul ( u [:, :, None ], du [:, None , :]) . view ( samples , - 1 ) return [ u_t ], [ theta ] library_deriv ( data , prediction , max_order ) [summary] Parameters: Name Type Description Default data Tensor [description] required prediction Tensor [description] required max_order int [description] required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Tuple[torch.Tensor, torch.Tensor]: [description] Source code in deepymod_torch/model/library.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def library_deriv ( data : torch . Tensor , prediction : torch . Tensor , max_order : int ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"[summary] Args: data (torch.Tensor): [description] prediction (torch.Tensor): [description] max_order (int): [description] Returns: Tuple[torch.Tensor, torch.Tensor]: [description] \"\"\" dy = grad ( prediction , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] time_deriv = dy [:, 0 : 1 ] if max_order == 0 : du = torch . ones_like ( time_deriv ) else : du = torch . cat (( torch . ones_like ( time_deriv ), dy [:, 1 : 2 ]), dim = 1 ) if max_order > 1 : for order in np . arange ( 1 , max_order ): du = torch . cat (( du , grad ( du [:, order : order + 1 ], data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ][:, 1 : 2 ]), dim = 1 ) return time_deriv , du library_poly ( prediction , max_order ) [summary] Parameters: Name Type Description Default prediction Tensor [description] required max_order int [description] required Returns: Type Description Tensor torch.Tensor: [description] Source code in deepymod_torch/model/library.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def library_poly ( prediction : torch . Tensor , max_order : int ) -> torch . Tensor : \"\"\"[summary] Args: prediction (torch.Tensor): [description] max_order (int): [description] Returns: torch.Tensor: [description] \"\"\" u = torch . ones_like ( prediction ) for order in np . arange ( 1 , max_order + 1 ): u = torch . cat (( u , u [:, order - 1 : order ] * prediction ), dim = 1 ) return u","title":"Libraries"},{"location":"api/lib/#deepymod_torch.model.library","text":"","title":"deepymod_torch.model.library"},{"location":"api/lib/#deepymod_torch.model.library.Library1D","text":"[summary] !!! args Library ([type]): [description]","title":"Library1D"},{"location":"api/lib/#deepymod_torch.model.library.Library1D.library","text":"[summary] Parameters: Name Type Description Default input Tuple[torch.Tensor, torch.Tensor] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod_torch/model/library.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def library ( self , input : Tuple [ torch . Tensor , torch . Tensor ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (Tuple[torch.Tensor, torch.Tensor]): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" prediction , data = input poly_list = [] deriv_list = [] time_deriv_list = [] # Creating lists for all outputs for output in np . arange ( prediction . shape [ 1 ]): time_deriv , du = library_deriv ( data , prediction [:, output : output + 1 ], self . diff_order ) u = library_poly ( prediction [:, output : output + 1 ], self . poly_order ) poly_list . append ( u ) deriv_list . append ( du ) time_deriv_list . append ( time_deriv ) samples = time_deriv_list [ 0 ] . shape [ 0 ] total_terms = poly_list [ 0 ] . shape [ 1 ] * deriv_list [ 0 ] . shape [ 1 ] # Calculating theta if len ( poly_list ) == 1 : # If we have a single output, we simply calculate and flatten matrix product # between polynomials and derivatives to get library theta = torch . matmul ( poly_list [ 0 ][:, :, None ], deriv_list [ 0 ][:, None , :]) . view ( samples , total_terms ) else : theta_uv = reduce (( lambda x , y : ( x [:, :, None ] @ y [:, None , :]) . view ( samples , - 1 )), poly_list ) # calculate all unique combinations of derivatives theta_dudv = torch . cat ([ torch . matmul ( du [:, :, None ], dv [:, None , :]) . view ( samples , - 1 )[:, 1 :] for du , dv in combinations ( deriv_list , 2 )], 1 ) theta = torch . cat ([ theta_uv , theta_dudv ], dim = 1 ) return time_deriv_list , [ theta ]","title":"library()"},{"location":"api/lib/#deepymod_torch.model.library.Library2D","text":"[summary] !!! args Library ([type]): [description]","title":"Library2D"},{"location":"api/lib/#deepymod_torch.model.library.Library2D.library","text":"[summary] Parameters: Name Type Description Default input Tuple[torch.Tensor, torch.Tensor] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod_torch/model/library.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def library ( self , input : Tuple [ torch . Tensor , torch . Tensor ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (Tuple[torch.Tensor, torch.Tensor]): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" prediction , data = input # Polynomial u = torch . ones_like ( prediction ) for order in np . arange ( 1 , self . poly_order + 1 ): u = torch . cat (( u , u [:, order - 1 : order ] * prediction ), dim = 1 ) # Gradients du = grad ( prediction , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] u_t = du [:, 0 : 1 ] u_x = du [:, 1 : 2 ] u_y = du [:, 2 : 3 ] du2 = grad ( u_x , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] u_xx = du2 [:, 1 : 2 ] u_xy = du2 [:, 2 : 3 ] u_yy = grad ( u_y , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ][:, 2 : 3 ] du = torch . cat (( torch . ones_like ( u_x ), u_x , u_y , u_xx , u_yy , u_xy ), dim = 1 ) samples = du . shape [ 0 ] # Bringing it together theta = torch . matmul ( u [:, :, None ], du [:, None , :]) . view ( samples , - 1 ) return [ u_t ], [ theta ]","title":"library()"},{"location":"api/lib/#deepymod_torch.model.library.library_deriv","text":"[summary] Parameters: Name Type Description Default data Tensor [description] required prediction Tensor [description] required max_order int [description] required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Tuple[torch.Tensor, torch.Tensor]: [description] Source code in deepymod_torch/model/library.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def library_deriv ( data : torch . Tensor , prediction : torch . Tensor , max_order : int ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"[summary] Args: data (torch.Tensor): [description] prediction (torch.Tensor): [description] max_order (int): [description] Returns: Tuple[torch.Tensor, torch.Tensor]: [description] \"\"\" dy = grad ( prediction , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] time_deriv = dy [:, 0 : 1 ] if max_order == 0 : du = torch . ones_like ( time_deriv ) else : du = torch . cat (( torch . ones_like ( time_deriv ), dy [:, 1 : 2 ]), dim = 1 ) if max_order > 1 : for order in np . arange ( 1 , max_order ): du = torch . cat (( du , grad ( du [:, order : order + 1 ], data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ][:, 1 : 2 ]), dim = 1 ) return time_deriv , du","title":"library_deriv()"},{"location":"api/lib/#deepymod_torch.model.library.library_poly","text":"[summary] Parameters: Name Type Description Default prediction Tensor [description] required max_order int [description] required Returns: Type Description Tensor torch.Tensor: [description] Source code in deepymod_torch/model/library.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def library_poly ( prediction : torch . Tensor , max_order : int ) -> torch . Tensor : \"\"\"[summary] Args: prediction (torch.Tensor): [description] max_order (int): [description] Returns: torch.Tensor: [description] \"\"\" u = torch . ones_like ( prediction ) for order in np . arange ( 1 , max_order + 1 ): u = torch . cat (( u , u [:, order - 1 : order ] * prediction ), dim = 1 ) return u","title":"library_poly()"},{"location":"api/sparse/","text":"Sparsity estimators which can be plugged into deepmod. We keep the API in line with scikit learn (mostly), so scikit learn can also be plugged in. See scikitlearn.linear_models for applicable estimators. Base [summary] !!! args Estimator ([type]): [description] fit ( self , X , y ) [summary] Parameters: Name Type Description Default X ndarray [description] required y ndarray [description] required Returns: Type Description ndarray np.ndarray: [description] Source code in deepymod_torch/model/sparse_estimators.py 28 29 30 31 32 33 34 35 36 37 38 39 def fit ( self , X : np . ndarray , y : np . ndarray ) -> np . ndarray : \"\"\"[summary] Args: X (np.ndarray): [description] y (np.ndarray): [description] Returns: np.ndarray: [description] \"\"\" coeffs = self . estimator . fit ( X , y ) . coef_ return coeffs Clustering Performs additional thresholding by clustering on coefficient result from estimator. Basically a thin wrapper around the given estimator. Results are fitted to two groups: components to keep and components to throw. fit ( self , X , y ) [summary] Parameters: Name Type Description Default X ndarray [description] required y ndarray [description] required Returns: Type Description ndarray np.ndarray: [description] Source code in deepymod_torch/model/sparse_estimators.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def fit ( self , X : np . ndarray , y : np . ndarray ) -> np . ndarray : \"\"\"[summary] Args: X (np.ndarray): [description] y (np.ndarray): [description] Returns: np.ndarray: [description] \"\"\" coeffs = self . estimator . fit ( X , y ) . coef_ [:, None ] # sklearn returns 1D clusters = self . kmeans . fit_predict ( np . abs ( coeffs )) . astype ( np . bool ) # make sure terms to keep are 1 and to remove are 0 max_idx = np . argmax ( np . abs ( coeffs )) if clusters [ max_idx ] != 1 : clusters = ~ clusters coeffs = clusters . astype ( np . float32 ) return coeffs PDEFIND Implements PDEFIND as a sparse estimator. fit ( self , X , y ) [summary] Parameters: Name Type Description Default X ndarray [description] required y ndarray [description] required Returns: Type Description ndarray np.ndarray: [description] Source code in deepymod_torch/model/sparse_estimators.py 110 111 112 113 114 115 116 117 118 119 120 121 def fit ( self , X : np . ndarray , y : np . ndarray ) -> np . ndarray : \"\"\"[summary] Args: X (np.ndarray): [description] y (np.ndarray): [description] Returns: np.ndarray: [description] \"\"\" coeffs = PDEFIND . TrainSTLSQ ( X , y [:, None ], self . lam , self . dtol ) return coeffs . squeeze () TrainSTLSQ ( X , y , alpha , delta_threshold , max_iterations = 100 , test_size = 0.2 , random_state = 0 ) staticmethod [summary] Parameters: Name Type Description Default X ndarray [description] required y ndarray [description] required alpha float [description] required delta_threshold float [description] required max_iterations int [description]. Defaults to 100. 100 test_size float [description]. Defaults to 0.2. 0.2 random_state int [description]. Defaults to 0. 0 Returns: Type Description ndarray np.ndarray: [description] Source code in deepymod_torch/model/sparse_estimators.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @staticmethod def TrainSTLSQ ( X : np . ndarray , y : np . ndarray , alpha : float , delta_threshold : float , max_iterations : int = 100 , test_size : float = 0.2 , random_state : int = 0 ) -> np . ndarray : \"\"\"[summary] Args: X (np.ndarray): [description] y (np.ndarray): [description] alpha (float): [description] delta_threshold (float): [description] max_iterations (int, optional): [description]. Defaults to 100. test_size (float, optional): [description]. Defaults to 0.2. random_state (int, optional): [description]. Defaults to 0. Returns: np.ndarray: [description] \"\"\" # Split data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = test_size , random_state = random_state ) # Set up the initial tolerance l0 penalty and estimates l0 = 1e-3 * np . linalg . cond ( X ) delta_t = delta_threshold # for interal use, can be updated # Initial estimate optimizer = STLSQ ( threshold = 0 , alpha = 0.0 , fit_intercept = False ) # Now similar to LSTSQ y_predict = optimizer . fit ( X_train , y_train ) . predict ( X_test ) min_loss = np . linalg . norm ( y_predict - y_test , 2 ) + l0 * np . count_nonzero ( optimizer . coef_ ) # Setting alpha and tolerance best_threshold = delta_t threshold = delta_t for iteration in np . arange ( max_iterations ): optimizer . set_params ( alpha = alpha , threshold = threshold ) y_predict = optimizer . fit ( X_train , y_train ) . predict ( X_test ) loss = np . linalg . norm ( y_predict - y_test , 2 ) + l0 * np . count_nonzero ( optimizer . coef_ ) if ( loss <= min_loss ) and not ( np . all ( optimizer . coef_ == 0 )): min_loss = loss best_threshold = threshold threshold += delta_threshold else : # if loss increases, we need to a) lower the current threshold and/or decrease step size new_lower_threshold = np . max ([ 0 , threshold - 2 * delta_t ]) delta_t = 2 * delta_t / ( max_iterations - iteration ) threshold = new_lower_threshold + delta_t optimizer . set_params ( alpha = alpha , threshold = best_threshold ) optimizer . fit ( X_train , y_train ) return optimizer . coef_ Threshold Performs additional thresholding on coefficient result from estimator. Basically a thin wrapper around the given estimator. fit ( self , X , y ) [summary] Parameters: Name Type Description Default X ndarray [description] required y ndarray [description] required Returns: Type Description ndarray np.ndarray: [description] Source code in deepymod_torch/model/sparse_estimators.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def fit ( self , X : np . ndarray , y : np . ndarray ) -> np . ndarray : \"\"\"[summary] Args: X (np.ndarray): [description] y (np.ndarray): [description] Returns: np.ndarray: [description] \"\"\" coeffs = self . estimator . fit ( X , y ) . coef_ coeffs [ np . abs ( coeffs ) < self . threshold ] = 0.0 return coeffs","title":"Sparsity"},{"location":"api/sparse/#deepymod_torch.model.sparse_estimators","text":"Sparsity estimators which can be plugged into deepmod. We keep the API in line with scikit learn (mostly), so scikit learn can also be plugged in. See scikitlearn.linear_models for applicable estimators.","title":"deepymod_torch.model.sparse_estimators"},{"location":"api/sparse/#deepymod_torch.model.sparse_estimators.Base","text":"[summary] !!! args Estimator ([type]): [description]","title":"Base"},{"location":"api/sparse/#deepymod_torch.model.sparse_estimators.Base.fit","text":"[summary] Parameters: Name Type Description Default X ndarray [description] required y ndarray [description] required Returns: Type Description ndarray np.ndarray: [description] Source code in deepymod_torch/model/sparse_estimators.py 28 29 30 31 32 33 34 35 36 37 38 39 def fit ( self , X : np . ndarray , y : np . ndarray ) -> np . ndarray : \"\"\"[summary] Args: X (np.ndarray): [description] y (np.ndarray): [description] Returns: np.ndarray: [description] \"\"\" coeffs = self . estimator . fit ( X , y ) . coef_ return coeffs","title":"fit()"},{"location":"api/sparse/#deepymod_torch.model.sparse_estimators.Clustering","text":"Performs additional thresholding by clustering on coefficient result from estimator. Basically a thin wrapper around the given estimator. Results are fitted to two groups: components to keep and components to throw.","title":"Clustering"},{"location":"api/sparse/#deepymod_torch.model.sparse_estimators.Clustering.fit","text":"[summary] Parameters: Name Type Description Default X ndarray [description] required y ndarray [description] required Returns: Type Description ndarray np.ndarray: [description] Source code in deepymod_torch/model/sparse_estimators.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def fit ( self , X : np . ndarray , y : np . ndarray ) -> np . ndarray : \"\"\"[summary] Args: X (np.ndarray): [description] y (np.ndarray): [description] Returns: np.ndarray: [description] \"\"\" coeffs = self . estimator . fit ( X , y ) . coef_ [:, None ] # sklearn returns 1D clusters = self . kmeans . fit_predict ( np . abs ( coeffs )) . astype ( np . bool ) # make sure terms to keep are 1 and to remove are 0 max_idx = np . argmax ( np . abs ( coeffs )) if clusters [ max_idx ] != 1 : clusters = ~ clusters coeffs = clusters . astype ( np . float32 ) return coeffs","title":"fit()"},{"location":"api/sparse/#deepymod_torch.model.sparse_estimators.PDEFIND","text":"Implements PDEFIND as a sparse estimator.","title":"PDEFIND"},{"location":"api/sparse/#deepymod_torch.model.sparse_estimators.PDEFIND.fit","text":"[summary] Parameters: Name Type Description Default X ndarray [description] required y ndarray [description] required Returns: Type Description ndarray np.ndarray: [description] Source code in deepymod_torch/model/sparse_estimators.py 110 111 112 113 114 115 116 117 118 119 120 121 def fit ( self , X : np . ndarray , y : np . ndarray ) -> np . ndarray : \"\"\"[summary] Args: X (np.ndarray): [description] y (np.ndarray): [description] Returns: np.ndarray: [description] \"\"\" coeffs = PDEFIND . TrainSTLSQ ( X , y [:, None ], self . lam , self . dtol ) return coeffs . squeeze ()","title":"fit()"},{"location":"api/sparse/#deepymod_torch.model.sparse_estimators.PDEFIND.TrainSTLSQ","text":"[summary] Parameters: Name Type Description Default X ndarray [description] required y ndarray [description] required alpha float [description] required delta_threshold float [description] required max_iterations int [description]. Defaults to 100. 100 test_size float [description]. Defaults to 0.2. 0.2 random_state int [description]. Defaults to 0. 0 Returns: Type Description ndarray np.ndarray: [description] Source code in deepymod_torch/model/sparse_estimators.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @staticmethod def TrainSTLSQ ( X : np . ndarray , y : np . ndarray , alpha : float , delta_threshold : float , max_iterations : int = 100 , test_size : float = 0.2 , random_state : int = 0 ) -> np . ndarray : \"\"\"[summary] Args: X (np.ndarray): [description] y (np.ndarray): [description] alpha (float): [description] delta_threshold (float): [description] max_iterations (int, optional): [description]. Defaults to 100. test_size (float, optional): [description]. Defaults to 0.2. random_state (int, optional): [description]. Defaults to 0. Returns: np.ndarray: [description] \"\"\" # Split data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = test_size , random_state = random_state ) # Set up the initial tolerance l0 penalty and estimates l0 = 1e-3 * np . linalg . cond ( X ) delta_t = delta_threshold # for interal use, can be updated # Initial estimate optimizer = STLSQ ( threshold = 0 , alpha = 0.0 , fit_intercept = False ) # Now similar to LSTSQ y_predict = optimizer . fit ( X_train , y_train ) . predict ( X_test ) min_loss = np . linalg . norm ( y_predict - y_test , 2 ) + l0 * np . count_nonzero ( optimizer . coef_ ) # Setting alpha and tolerance best_threshold = delta_t threshold = delta_t for iteration in np . arange ( max_iterations ): optimizer . set_params ( alpha = alpha , threshold = threshold ) y_predict = optimizer . fit ( X_train , y_train ) . predict ( X_test ) loss = np . linalg . norm ( y_predict - y_test , 2 ) + l0 * np . count_nonzero ( optimizer . coef_ ) if ( loss <= min_loss ) and not ( np . all ( optimizer . coef_ == 0 )): min_loss = loss best_threshold = threshold threshold += delta_threshold else : # if loss increases, we need to a) lower the current threshold and/or decrease step size new_lower_threshold = np . max ([ 0 , threshold - 2 * delta_t ]) delta_t = 2 * delta_t / ( max_iterations - iteration ) threshold = new_lower_threshold + delta_t optimizer . set_params ( alpha = alpha , threshold = best_threshold ) optimizer . fit ( X_train , y_train ) return optimizer . coef_","title":"TrainSTLSQ()"},{"location":"api/sparse/#deepymod_torch.model.sparse_estimators.Threshold","text":"Performs additional thresholding on coefficient result from estimator. Basically a thin wrapper around the given estimator.","title":"Threshold"},{"location":"api/sparse/#deepymod_torch.model.sparse_estimators.Threshold.fit","text":"[summary] Parameters: Name Type Description Default X ndarray [description] required y ndarray [description] required Returns: Type Description ndarray np.ndarray: [description] Source code in deepymod_torch/model/sparse_estimators.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def fit ( self , X : np . ndarray , y : np . ndarray ) -> np . ndarray : \"\"\"[summary] Args: X (np.ndarray): [description] y (np.ndarray): [description] Returns: np.ndarray: [description] \"\"\" coeffs = self . estimator . fit ( X , y ) . coef_ coeffs [ np . abs ( coeffs ) < self . threshold ] = 0.0 return coeffs","title":"fit()"},{"location":"api/training/","text":"convergence Convergence Implements convergence criterium. Convergence is when change in patience epochs is smaller than delta. __call__ ( self , epoch , l1_norm ) special [summary] Parameters: Name Type Description Default epoch int [description] required l1_norm Tensor [description] required Source code in deepymod_torch/training/convergence.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def __call__ ( self , epoch : int , l1_norm : torch . Tensor ) -> None : \"\"\"[summary] Args: epoch (int): [description] l1_norm (torch.Tensor): [description] \"\"\" if self . start_l1 is None : self . start_l1 = l1_norm elif torch . abs ( self . start_l1 - l1_norm ) . item () < self . delta : self . counter += 1 if self . counter >= self . patience : self . converged = True else : self . start_l1 = l1_norm self . counter = 0 sparsity_scheduler Periodic Controls when to apply sparsity. Initial_epoch is first time of appliance, then every periodicity epochs. __call__ ( self , iteration , l1_norm ) special [summary] Parameters: Name Type Description Default iteration int [description] required l1_norm Tensor [description] required Source code in deepymod_torch/training/sparsity_scheduler.py 14 15 16 17 18 19 20 21 22 23 def __call__ ( self , iteration : int , l1_norm : torch . Tensor ) -> None : \"\"\"[summary] Args: iteration (int): [description] l1_norm (torch.Tensor): [description] \"\"\" if iteration >= self . initial_epoch : if ( iteration - self . initial_epoch ) % self . periodicity == 0 : self . apply_sparsity = True reset ( self ) [summary] Source code in deepymod_torch/training/sparsity_scheduler.py 25 26 27 28 def reset ( self ) -> None : \"\"\"[summary] \"\"\" self . apply_sparsity = False training train ( model , data , target , optimizer , sparsity_scheduler , log_dir = None , max_iterations = 10000 , ** convergence_kwargs ) [summary] Parameters: Name Type Description Default model DeepMoD [description] required data Tensor [description] required target Tensor [description] required optimizer [type] [description] required sparsity_scheduler [type] [description] required log_dir Optional[str] [description]. Defaults to None. None max_iterations int [description]. Defaults to 10000. 10000 Source code in deepymod_torch/training/training.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def train ( model : DeepMoD , data : torch . Tensor , target : torch . Tensor , optimizer , sparsity_scheduler , log_dir : Optional [ str ] = None , max_iterations : int = 10000 , ** convergence_kwargs ) -> None : \"\"\"[summary] Args: model (DeepMoD): [description] data (torch.Tensor): [description] target (torch.Tensor): [description] optimizer ([type]): [description] sparsity_scheduler ([type]): [description] log_dir (Optional[str], optional): [description]. Defaults to None. max_iterations (int, optional): [description]. Defaults to 10000. \"\"\" start_time = time . time () number_of_terms = [ coeff_vec . shape [ 0 ] for coeff_vec in model ( data )[ 3 ]] board = Tensorboard ( number_of_terms , log_dir ) # initializing custom tb board # Training convergence = Convergence ( ** convergence_kwargs ) print ( '| Iteration | Progress | Time remaining | Loss | MSE | Reg | L1 norm |' ) for iteration in np . arange ( 0 , max_iterations + 1 ): # ================== Training Model ============================ prediction , time_derivs , sparse_thetas , thetas , constraint_coeffs = model ( data ) MSE = torch . mean (( prediction - target ) ** 2 , dim = 0 ) # loss per output Reg = torch . stack ([ torch . mean (( dt - theta @ coeff_vector ) ** 2 ) for dt , theta , coeff_vector in zip ( time_derivs , sparse_thetas , constraint_coeffs )]) loss = torch . sum ( 2 * torch . log ( 2 * pi * MSE ) + Reg / ( MSE + 1e-6 )) # 1e-5 for numerical stability # Optimizer step optimizer . zero_grad () loss . backward () optimizer . step () # ====================== Logging ======================= # Write progress to command line and tensorboard l1_norm = torch . stack ([ torch . sum ( torch . abs ( coeff_vector )) for coeff_vector in constraint_coeffs ]) if iteration % 25 == 0 : progress ( iteration , start_time , max_iterations , loss . item (), torch . sum ( MSE ) . item (), torch . sum ( Reg ) . item (), torch . sum ( l1_norm ) . item ()) # We pad the sparse vectors with zeros so they get written correctly) constraint_coeff_vectors = [ torch . zeros ( mask . size ()) . masked_scatter_ ( mask , coeff_vector . detach () . squeeze ()) for mask , coeff_vector in zip ( model . constraint . sparsity_masks , constraint_coeffs )] unscaled_constraint_coeff_vectors = [ torch . zeros ( mask . size ()) . masked_scatter_ ( mask , coeff_vector . detach () . squeeze ()) / norm . squeeze () for mask , coeff_vector , norm in zip ( model . constraint . sparsity_masks , constraint_coeffs , model . library . norms )] board . write ( iteration , loss , MSE , Reg , l1_norm , constraint_coeff_vectors , unscaled_constraint_coeff_vectors ) # ================== Validation and sparsity ============= # Updating sparsity and or convergence sparsity_scheduler ( iteration , torch . sum ( l1_norm )) if sparsity_scheduler . apply_sparsity is True : with torch . no_grad (): model . constraint . sparsity_masks = model . sparse_estimator ( thetas , time_derivs ) sparsity_scheduler . reset () print ( model . constraint . sparsity_masks ) # Checking convergence convergence ( iteration , torch . sum ( l1_norm )) if convergence . converged is True : print ( 'Algorithm converged. Stopping training.' ) break board . close ()","title":"Training"},{"location":"api/training/#deepymod_torch.training","text":"","title":"deepymod_torch.training"},{"location":"api/training/#deepymod_torch.training.convergence","text":"","title":"convergence"},{"location":"api/training/#deepymod_torch.training.convergence.Convergence","text":"Implements convergence criterium. Convergence is when change in patience epochs is smaller than delta.","title":"Convergence"},{"location":"api/training/#deepymod_torch.training.convergence.Convergence.__call__","text":"[summary] Parameters: Name Type Description Default epoch int [description] required l1_norm Tensor [description] required Source code in deepymod_torch/training/convergence.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def __call__ ( self , epoch : int , l1_norm : torch . Tensor ) -> None : \"\"\"[summary] Args: epoch (int): [description] l1_norm (torch.Tensor): [description] \"\"\" if self . start_l1 is None : self . start_l1 = l1_norm elif torch . abs ( self . start_l1 - l1_norm ) . item () < self . delta : self . counter += 1 if self . counter >= self . patience : self . converged = True else : self . start_l1 = l1_norm self . counter = 0","title":"__call__()"},{"location":"api/training/#deepymod_torch.training.sparsity_scheduler","text":"","title":"sparsity_scheduler"},{"location":"api/training/#deepymod_torch.training.sparsity_scheduler.Periodic","text":"Controls when to apply sparsity. Initial_epoch is first time of appliance, then every periodicity epochs.","title":"Periodic"},{"location":"api/training/#deepymod_torch.training.sparsity_scheduler.Periodic.__call__","text":"[summary] Parameters: Name Type Description Default iteration int [description] required l1_norm Tensor [description] required Source code in deepymod_torch/training/sparsity_scheduler.py 14 15 16 17 18 19 20 21 22 23 def __call__ ( self , iteration : int , l1_norm : torch . Tensor ) -> None : \"\"\"[summary] Args: iteration (int): [description] l1_norm (torch.Tensor): [description] \"\"\" if iteration >= self . initial_epoch : if ( iteration - self . initial_epoch ) % self . periodicity == 0 : self . apply_sparsity = True","title":"__call__()"},{"location":"api/training/#deepymod_torch.training.sparsity_scheduler.Periodic.reset","text":"[summary] Source code in deepymod_torch/training/sparsity_scheduler.py 25 26 27 28 def reset ( self ) -> None : \"\"\"[summary] \"\"\" self . apply_sparsity = False","title":"reset()"},{"location":"api/training/#deepymod_torch.training.training","text":"","title":"training"},{"location":"api/training/#deepymod_torch.training.training.train","text":"[summary] Parameters: Name Type Description Default model DeepMoD [description] required data Tensor [description] required target Tensor [description] required optimizer [type] [description] required sparsity_scheduler [type] [description] required log_dir Optional[str] [description]. Defaults to None. None max_iterations int [description]. Defaults to 10000. 10000 Source code in deepymod_torch/training/training.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def train ( model : DeepMoD , data : torch . Tensor , target : torch . Tensor , optimizer , sparsity_scheduler , log_dir : Optional [ str ] = None , max_iterations : int = 10000 , ** convergence_kwargs ) -> None : \"\"\"[summary] Args: model (DeepMoD): [description] data (torch.Tensor): [description] target (torch.Tensor): [description] optimizer ([type]): [description] sparsity_scheduler ([type]): [description] log_dir (Optional[str], optional): [description]. Defaults to None. max_iterations (int, optional): [description]. Defaults to 10000. \"\"\" start_time = time . time () number_of_terms = [ coeff_vec . shape [ 0 ] for coeff_vec in model ( data )[ 3 ]] board = Tensorboard ( number_of_terms , log_dir ) # initializing custom tb board # Training convergence = Convergence ( ** convergence_kwargs ) print ( '| Iteration | Progress | Time remaining | Loss | MSE | Reg | L1 norm |' ) for iteration in np . arange ( 0 , max_iterations + 1 ): # ================== Training Model ============================ prediction , time_derivs , sparse_thetas , thetas , constraint_coeffs = model ( data ) MSE = torch . mean (( prediction - target ) ** 2 , dim = 0 ) # loss per output Reg = torch . stack ([ torch . mean (( dt - theta @ coeff_vector ) ** 2 ) for dt , theta , coeff_vector in zip ( time_derivs , sparse_thetas , constraint_coeffs )]) loss = torch . sum ( 2 * torch . log ( 2 * pi * MSE ) + Reg / ( MSE + 1e-6 )) # 1e-5 for numerical stability # Optimizer step optimizer . zero_grad () loss . backward () optimizer . step () # ====================== Logging ======================= # Write progress to command line and tensorboard l1_norm = torch . stack ([ torch . sum ( torch . abs ( coeff_vector )) for coeff_vector in constraint_coeffs ]) if iteration % 25 == 0 : progress ( iteration , start_time , max_iterations , loss . item (), torch . sum ( MSE ) . item (), torch . sum ( Reg ) . item (), torch . sum ( l1_norm ) . item ()) # We pad the sparse vectors with zeros so they get written correctly) constraint_coeff_vectors = [ torch . zeros ( mask . size ()) . masked_scatter_ ( mask , coeff_vector . detach () . squeeze ()) for mask , coeff_vector in zip ( model . constraint . sparsity_masks , constraint_coeffs )] unscaled_constraint_coeff_vectors = [ torch . zeros ( mask . size ()) . masked_scatter_ ( mask , coeff_vector . detach () . squeeze ()) / norm . squeeze () for mask , coeff_vector , norm in zip ( model . constraint . sparsity_masks , constraint_coeffs , model . library . norms )] board . write ( iteration , loss , MSE , Reg , l1_norm , constraint_coeff_vectors , unscaled_constraint_coeff_vectors ) # ================== Validation and sparsity ============= # Updating sparsity and or convergence sparsity_scheduler ( iteration , torch . sum ( l1_norm )) if sparsity_scheduler . apply_sparsity is True : with torch . no_grad (): model . constraint . sparsity_masks = model . sparse_estimator ( thetas , time_derivs ) sparsity_scheduler . reset () print ( model . constraint . sparsity_masks ) # Checking convergence convergence ( iteration , torch . sum ( l1_norm )) if convergence . converged is True : print ( 'Algorithm converged. Stopping training.' ) break board . close ()","title":"train()"},{"location":"api/utils/","text":"We also make a package called phimal-utilities.","title":"Phimal-Utilities"},{"location":"background/deepmod/","text":"Deepmod Deepmod is really cool bla bla. f(x) = x^2 f(x) = x^2 of inline f(x) f(x) .","title":"DeepMoD"},{"location":"background/deepmod/#deepmod","text":"Deepmod is really cool bla bla. f(x) = x^2 f(x) = x^2 of inline f(x) f(x) .","title":"Deepmod"},{"location":"background/group_sparsity/","text":"Group sparsity","title":"Group sparsity"},{"location":"background/group_sparsity/#group-sparsity","text":"","title":"Group sparsity"},{"location":"examples/Burgers/PDE_Burgers/","text":"Example Burgers' equation In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the Burgers' equation. We start by importing the required libraries and setting the plotting style: # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD stuff from deepymod_torch.DeepMod import DeepMod from deepymod_torch.library_functions import library_1D_in from deepymod_torch.training import train_deepmod, train_mse # Settings for reproducibility np.random.seed(42) torch.manual_seed(0) %load_ext autoreload %autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Next, we prepare the dataset. data = np.load('data/burgers.npy', allow_pickle=True).item() print('Shape of grid:', data['x'].shape) Shape of grid: (256, 101) Let's plot it to get an idea of the data: fig, ax = plt.subplots() im = ax.contourf(data['x'], data['t'], np.real(data['u'])) ax.set_xlabel('x') ax.set_ylabel('t') fig.colorbar(mappable=im) plt.show() X = np.transpose((data['t'].flatten(), data['x'].flatten())) y = np.real(data['u']).reshape((data['u'].size, 1)) print(X.shape, y.shape) (25856, 2) (25856, 1) As we can see, X X has 2 dimensions, \\{x, t\\} \\{x, t\\} , while y y has only one, \\{u\\} \\{u\\} . Always explicity set the shape (i.e. N\\times 1 N\\times 1 , not N N ) or you'll get errors. This dataset is noiseless, so let's add 5\\% 5\\% noise: noise_level = 0.05 y_noisy = y + noise_level * np.std(y) * np.random.randn(y[:,0].size, 1) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 1000 idx = np.random.permutation(y.shape[0]) X_train = torch.tensor(X[idx, :][:number_of_samples], dtype=torch.float32, requires_grad=True) y_train = torch.tensor(y_noisy[idx, :][:number_of_samples], dtype=torch.float32) print(X_train.shape, y_train.shape) torch.Size([1000, 2]) torch.Size([1000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig, axes = plt.subplots(ncols=3, figsize=(15, 4)) im0 = axes[0].contourf(data['x'], data['t'], np.real(data['u']), cmap='coolwarm') axes[0].set_xlabel('x') axes[0].set_ylabel('t') axes[0].set_title('Ground truth') im1 = axes[1].contourf(data['x'], data['t'], y_noisy.reshape(data['x'].shape), cmap='coolwarm') axes[1].set_xlabel('x') axes[1].set_title('Noisy') sampled = np.array([y_noisy[index, 0] if index in idx[:number_of_samples] else np.nan for index in np.arange(data['x'].size)]) sampled = np.rot90(sampled.reshape(data['x'].shape)) #array needs to be rotated because of imshow im2 = axes[2].imshow(sampled, aspect='auto', cmap='coolwarm') axes[2].set_xlabel('x') axes[2].set_title('Sampled') fig.colorbar(im1, ax=axes.ravel().tolist()) plt.show() Configuring DeepMoD We now setup the options for DeepMoD. The setup requires the dimensions of the neural network, a library function and some args for the library function: ## Running DeepMoD config = {'n_in': 2, 'hidden_dims': [20, 20, 20, 20, 20, 20], 'n_out': 1, 'library_function': library_1D_in, 'library_args':{'poly_order': 1, 'diff_order': 2}} Now we instantiate the model: model = DeepMod(**config) optimizer = torch.optim.Adam([{'params': model.network_parameters(), 'lr':0.001}, {'params': model.coeff_vector(), 'lr':0.005}]) Run DeepMoD We can now run DeepMoD using all the options we have set and the training data. We need to slightly preprocess the input data for the derivatives: train_deepmod(model, X_train, y_train, optimizer, 25000, {'l1': 1e-5}) | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 100 0.40% 371s 2.29e-02 1.64e-02 6.41e-03 1.25e-04 --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-21-790b1a88b6d5> in <module> ----> 1 train_deepmod(model, X_train, y_train, optimizer, 25000, {'l1': 1e-5}) ~/Documents/GitHub/New_DeepMod_Simple/DeePyMoD_torch/src/deepymod_torch/training.py in train_deepmod(model, data, target, optimizer, max_iterations, loss_func_args) 67 '''Performs full deepmod cycle: trains model, thresholds and trains again for unbiased estimate. Updates model in-place.''' 68 # Train first cycle and get prediction ---> 69 train(model, data, target, optimizer, max_iterations, loss_func_args) 70 prediction, time_deriv_list, sparse_theta_list, coeff_vector_list = model(data) 71 ~/Documents/GitHub/New_DeepMod_Simple/DeePyMoD_torch/src/deepymod_torch/training.py in train(model, data, target, optimizer, max_iterations, loss_func_args) 32 # Optimizer step 33 optimizer.zero_grad() ---> 34 loss.backward() 35 optimizer.step() 36 board.close() ~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph) 193 products. Defaults to ``False``. 194 \"\"\" --> 195 torch.autograd.backward(self, gradient, retain_graph, create_graph) 196 197 def register_hook(self, hook): ~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables) 97 Variable._execution_engine.run_backward( 98 tensors, grad_tensors, retain_graph, create_graph, ---> 99 allow_unreachable=True) # allow_unreachable flag 100 101 KeyboardInterrupt: Now that DeepMoD has converged, it has found the following numbers: print(model.fit.sparsity_mask) [tensor([2, 4])] print(model.fit.coeff_vector[0]) Parameter containing: tensor([[ 0.0974], [-0.9905]], requires_grad=True)","title":"Burgers"},{"location":"examples/Burgers/PDE_Burgers/#example-burgers-equation","text":"In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the Burgers' equation. We start by importing the required libraries and setting the plotting style: # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD stuff from deepymod_torch.DeepMod import DeepMod from deepymod_torch.library_functions import library_1D_in from deepymod_torch.training import train_deepmod, train_mse # Settings for reproducibility np.random.seed(42) torch.manual_seed(0) %load_ext autoreload %autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Next, we prepare the dataset. data = np.load('data/burgers.npy', allow_pickle=True).item() print('Shape of grid:', data['x'].shape) Shape of grid: (256, 101) Let's plot it to get an idea of the data: fig, ax = plt.subplots() im = ax.contourf(data['x'], data['t'], np.real(data['u'])) ax.set_xlabel('x') ax.set_ylabel('t') fig.colorbar(mappable=im) plt.show() X = np.transpose((data['t'].flatten(), data['x'].flatten())) y = np.real(data['u']).reshape((data['u'].size, 1)) print(X.shape, y.shape) (25856, 2) (25856, 1) As we can see, X X has 2 dimensions, \\{x, t\\} \\{x, t\\} , while y y has only one, \\{u\\} \\{u\\} . Always explicity set the shape (i.e. N\\times 1 N\\times 1 , not N N ) or you'll get errors. This dataset is noiseless, so let's add 5\\% 5\\% noise: noise_level = 0.05 y_noisy = y + noise_level * np.std(y) * np.random.randn(y[:,0].size, 1) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 1000 idx = np.random.permutation(y.shape[0]) X_train = torch.tensor(X[idx, :][:number_of_samples], dtype=torch.float32, requires_grad=True) y_train = torch.tensor(y_noisy[idx, :][:number_of_samples], dtype=torch.float32) print(X_train.shape, y_train.shape) torch.Size([1000, 2]) torch.Size([1000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig, axes = plt.subplots(ncols=3, figsize=(15, 4)) im0 = axes[0].contourf(data['x'], data['t'], np.real(data['u']), cmap='coolwarm') axes[0].set_xlabel('x') axes[0].set_ylabel('t') axes[0].set_title('Ground truth') im1 = axes[1].contourf(data['x'], data['t'], y_noisy.reshape(data['x'].shape), cmap='coolwarm') axes[1].set_xlabel('x') axes[1].set_title('Noisy') sampled = np.array([y_noisy[index, 0] if index in idx[:number_of_samples] else np.nan for index in np.arange(data['x'].size)]) sampled = np.rot90(sampled.reshape(data['x'].shape)) #array needs to be rotated because of imshow im2 = axes[2].imshow(sampled, aspect='auto', cmap='coolwarm') axes[2].set_xlabel('x') axes[2].set_title('Sampled') fig.colorbar(im1, ax=axes.ravel().tolist()) plt.show()","title":"Example Burgers' equation"},{"location":"examples/Burgers/PDE_Burgers/#configuring-deepmod","text":"We now setup the options for DeepMoD. The setup requires the dimensions of the neural network, a library function and some args for the library function: ## Running DeepMoD config = {'n_in': 2, 'hidden_dims': [20, 20, 20, 20, 20, 20], 'n_out': 1, 'library_function': library_1D_in, 'library_args':{'poly_order': 1, 'diff_order': 2}} Now we instantiate the model: model = DeepMod(**config) optimizer = torch.optim.Adam([{'params': model.network_parameters(), 'lr':0.001}, {'params': model.coeff_vector(), 'lr':0.005}])","title":"Configuring DeepMoD"},{"location":"examples/Burgers/PDE_Burgers/#run-deepmod","text":"We can now run DeepMoD using all the options we have set and the training data. We need to slightly preprocess the input data for the derivatives: train_deepmod(model, X_train, y_train, optimizer, 25000, {'l1': 1e-5}) | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 100 0.40% 371s 2.29e-02 1.64e-02 6.41e-03 1.25e-04 --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-21-790b1a88b6d5> in <module> ----> 1 train_deepmod(model, X_train, y_train, optimizer, 25000, {'l1': 1e-5}) ~/Documents/GitHub/New_DeepMod_Simple/DeePyMoD_torch/src/deepymod_torch/training.py in train_deepmod(model, data, target, optimizer, max_iterations, loss_func_args) 67 '''Performs full deepmod cycle: trains model, thresholds and trains again for unbiased estimate. Updates model in-place.''' 68 # Train first cycle and get prediction ---> 69 train(model, data, target, optimizer, max_iterations, loss_func_args) 70 prediction, time_deriv_list, sparse_theta_list, coeff_vector_list = model(data) 71 ~/Documents/GitHub/New_DeepMod_Simple/DeePyMoD_torch/src/deepymod_torch/training.py in train(model, data, target, optimizer, max_iterations, loss_func_args) 32 # Optimizer step 33 optimizer.zero_grad() ---> 34 loss.backward() 35 optimizer.step() 36 board.close() ~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph) 193 products. Defaults to ``False``. 194 \"\"\" --> 195 torch.autograd.backward(self, gradient, retain_graph, create_graph) 196 197 def register_hook(self, hook): ~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables) 97 Variable._execution_engine.run_backward( 98 tensors, grad_tensors, retain_graph, create_graph, ---> 99 allow_unreachable=True) # allow_unreachable flag 100 101 KeyboardInterrupt: Now that DeepMoD has converged, it has found the following numbers: print(model.fit.sparsity_mask) [tensor([2, 4])] print(model.fit.coeff_vector[0]) Parameter containing: tensor([[ 0.0974], [-0.9905]], requires_grad=True)","title":"Run DeepMoD"},{"location":"examples/kdv/PDE_KdV/","text":"Example Korteweg de Vries equation In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the Burgers' equation. We start by importing the required libraries and setting the plotting style: # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD stuff from deepymod_torch.DeepMod import DeepMod from deepymod_torch.library_functions import library_1D_in from deepymod_torch.training import train_deepmod, train_mse # Settings for reproducibility np.random.seed(42) torch.manual_seed(0) %load_ext autoreload %autoreload 2 Next, we prepare the dataset. data = np.load('data/kdv.npy', allow_pickle=True).item() print('Shape of grid:', data['x'].shape) Shape of grid: (512, 201) Let's plot it to get an idea of the data: fig, ax = plt.subplots() im = ax.contourf(data['x'], data['t'], np.real(data['u'])) ax.set_xlabel('x') ax.set_ylabel('t') fig.colorbar(mappable=im) plt.show() X = np.transpose((data['t'].flatten(), data['x'].flatten())) y = np.real(data['u']).reshape((data['u'].size, 1)) print(X.shape, y.shape) (102912, 2) (102912, 1) As we can see, X X has 2 dimensions, \\{x, t\\} \\{x, t\\} , while y y has only one, \\{u\\} \\{u\\} . Always explicity set the shape (i.e. N\\times 1 N\\times 1 , not N N ) or you'll get errors. This dataset is noiseless, so let's add 5\\% 5\\% noise: noise_level = 0.05 y_noisy = y + noise_level * np.std(y) * np.random.randn(y[:,0].size, 1) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 1000 idx = np.random.permutation(y.shape[0]) X_train = torch.tensor(X[idx, :][:number_of_samples], dtype=torch.float32, requires_grad=True) y_train = torch.tensor(y_noisy[idx, :][:number_of_samples], dtype=torch.float32) print(X_train.shape, y_train.shape) torch.Size([1000, 2]) torch.Size([1000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig, axes = plt.subplots(ncols=3, figsize=(15, 4)) im0 = axes[0].contourf(data['x'], data['t'], np.real(data['u']), cmap='coolwarm') axes[0].set_xlabel('x') axes[0].set_ylabel('t') axes[0].set_title('Ground truth') im1 = axes[1].contourf(data['x'], data['t'], y_noisy.reshape(data['x'].shape), cmap='coolwarm') axes[1].set_xlabel('x') axes[1].set_title('Noisy') sampled = np.array([y_noisy[index, 0] if index in idx[:number_of_samples] else np.nan for index in np.arange(data['x'].size)]) sampled = np.rot90(sampled.reshape(data['x'].shape)) #array needs to be rotated because of imshow im2 = axes[2].imshow(sampled, aspect='auto', cmap='coolwarm') axes[2].set_xlabel('x') axes[2].set_title('Sampled') fig.colorbar(im1, ax=axes.ravel().tolist()) plt.show() Configuring DeepMoD We now setup the options for DeepMoD. The setup requires the dimensions of the neural network, a library function and some args for the library function: ## Running DeepMoD config = {'n_in': 2, 'hidden_dims': [20, 20, 20, 20, 20, 20], 'n_out': 1, 'library_function': library_1D_in, 'library_args':{'poly_order': 1, 'diff_order': 3}} Now we instantiate the model: model = DeepMod(**config) optimizer = torch.optim.Adam([{'params': model.network_parameters(), 'lr':0.001}, {'params': model.coeff_vector(), 'lr':0.0025}],betas=(0.99, 0.99)) Run DeepMoD We can now run DeepMoD using all the options we have set and the training data. We need to slightly preprocess the input data for the derivatives: train_deepmod(model, X_train, y_train, optimizer, 25000, {'l1': 1e-5}) | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 25000 100.00% 0s 5.83e-05 3.13e-05 4.58e-06 2.24e-05 [Parameter containing: tensor([[-0.7742], [-4.7608]], requires_grad=True)] [tensor([3, 5])] | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 25000 100.00% 0s 9.18e-05 2.87e-05 6.32e-05 0.00e+00 Now that DeepMoD has converged, it has found the following numbers: print(model.fit.sparsity_mask) [tensor([3, 5])] print(model.fit.coeff_vector[0]) Parameter containing: tensor([[-0.7742], [-4.7608]], requires_grad=True)","title":"Korteweg-de Vries"},{"location":"examples/kdv/PDE_KdV/#example-korteweg-de-vries-equation","text":"In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the Burgers' equation. We start by importing the required libraries and setting the plotting style: # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD stuff from deepymod_torch.DeepMod import DeepMod from deepymod_torch.library_functions import library_1D_in from deepymod_torch.training import train_deepmod, train_mse # Settings for reproducibility np.random.seed(42) torch.manual_seed(0) %load_ext autoreload %autoreload 2 Next, we prepare the dataset. data = np.load('data/kdv.npy', allow_pickle=True).item() print('Shape of grid:', data['x'].shape) Shape of grid: (512, 201) Let's plot it to get an idea of the data: fig, ax = plt.subplots() im = ax.contourf(data['x'], data['t'], np.real(data['u'])) ax.set_xlabel('x') ax.set_ylabel('t') fig.colorbar(mappable=im) plt.show() X = np.transpose((data['t'].flatten(), data['x'].flatten())) y = np.real(data['u']).reshape((data['u'].size, 1)) print(X.shape, y.shape) (102912, 2) (102912, 1) As we can see, X X has 2 dimensions, \\{x, t\\} \\{x, t\\} , while y y has only one, \\{u\\} \\{u\\} . Always explicity set the shape (i.e. N\\times 1 N\\times 1 , not N N ) or you'll get errors. This dataset is noiseless, so let's add 5\\% 5\\% noise: noise_level = 0.05 y_noisy = y + noise_level * np.std(y) * np.random.randn(y[:,0].size, 1) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 1000 idx = np.random.permutation(y.shape[0]) X_train = torch.tensor(X[idx, :][:number_of_samples], dtype=torch.float32, requires_grad=True) y_train = torch.tensor(y_noisy[idx, :][:number_of_samples], dtype=torch.float32) print(X_train.shape, y_train.shape) torch.Size([1000, 2]) torch.Size([1000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig, axes = plt.subplots(ncols=3, figsize=(15, 4)) im0 = axes[0].contourf(data['x'], data['t'], np.real(data['u']), cmap='coolwarm') axes[0].set_xlabel('x') axes[0].set_ylabel('t') axes[0].set_title('Ground truth') im1 = axes[1].contourf(data['x'], data['t'], y_noisy.reshape(data['x'].shape), cmap='coolwarm') axes[1].set_xlabel('x') axes[1].set_title('Noisy') sampled = np.array([y_noisy[index, 0] if index in idx[:number_of_samples] else np.nan for index in np.arange(data['x'].size)]) sampled = np.rot90(sampled.reshape(data['x'].shape)) #array needs to be rotated because of imshow im2 = axes[2].imshow(sampled, aspect='auto', cmap='coolwarm') axes[2].set_xlabel('x') axes[2].set_title('Sampled') fig.colorbar(im1, ax=axes.ravel().tolist()) plt.show()","title":"Example Korteweg de Vries equation"},{"location":"examples/kdv/PDE_KdV/#configuring-deepmod","text":"We now setup the options for DeepMoD. The setup requires the dimensions of the neural network, a library function and some args for the library function: ## Running DeepMoD config = {'n_in': 2, 'hidden_dims': [20, 20, 20, 20, 20, 20], 'n_out': 1, 'library_function': library_1D_in, 'library_args':{'poly_order': 1, 'diff_order': 3}} Now we instantiate the model: model = DeepMod(**config) optimizer = torch.optim.Adam([{'params': model.network_parameters(), 'lr':0.001}, {'params': model.coeff_vector(), 'lr':0.0025}],betas=(0.99, 0.99))","title":"Configuring DeepMoD"},{"location":"examples/kdv/PDE_KdV/#run-deepmod","text":"We can now run DeepMoD using all the options we have set and the training data. We need to slightly preprocess the input data for the derivatives: train_deepmod(model, X_train, y_train, optimizer, 25000, {'l1': 1e-5}) | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 25000 100.00% 0s 5.83e-05 3.13e-05 4.58e-06 2.24e-05 [Parameter containing: tensor([[-0.7742], [-4.7608]], requires_grad=True)] [tensor([3, 5])] | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 25000 100.00% 0s 9.18e-05 2.87e-05 6.32e-05 0.00e+00 Now that DeepMoD has converged, it has found the following numbers: print(model.fit.sparsity_mask) [tensor([3, 5])] print(model.fit.coeff_vector[0]) Parameter containing: tensor([[-0.7742], [-4.7608]], requires_grad=True)","title":"Run DeepMoD"}]}